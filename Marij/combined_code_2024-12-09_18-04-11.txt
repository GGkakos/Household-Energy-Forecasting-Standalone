##### DIRECTORY TREE #####
==================================================
./
    scratchpad.py
    arima2.ipynb
    arima3.py
    lstm1.py
    arima.ipynb
    Archive/
        household_data_v1.py
        household_data_v2.py
        room_summary2.csv
        sensor_stats.csv
        room_summary.py
        room_summary2.py
        sensor_min_max.csv
        merge2.py
        sensor_stats_summary.csv
        room_summary.csv
        summary2.csv
        sensor_stats_2.py
        Append_approach.xlsx
        sensor_summary_combined.csv
        extract_csv.py
        merge1.py
        summary2.py
        summary.csv
        Book1.xlsx
        summary1.py
        merge3.py
    charts/
        home_home263_gas_residuals.png
        Primary_Dataset_elec_residuals.png
        Primary_Dataset_elec_lstm_fitted_vs_actual.png
        home_home263_elec_fitted_vs_actual.png
        home_home263_gas_fitted_vs_actual.png
        Primary_Dataset_gas_residuals.png
        home_home304_gas_fitted_vs_actual.png
        home_home147_elec_residuals.png
        home_home304_elec_residuals.png
        Primary_Dataset_gas_lstm_residuals.png
        Primary_Dataset_elec_lstm_residuals.png
        home_home263_elec_residuals.png
        home_home304_gas_residuals.png
        home_home304_elec_fitted_vs_actual.png
        Primary_Dataset_elec_fitted_vs_actual.png
        Primary_Dataset_gas_fitted_vs_actual.png
        Primary_Dataset_gas_lstm_fitted_vs_actual.png
        home_home147_elec_fitted_vs_actual.png
    ind_homes/
        home_home306.csv
        home_home117.csv
        home_home206.csv
        home_home160.csv
        home_home216.csv
        home_home139.csv
        home_home232.csv
        home_home76.csv
        home_home274.csv
        home_home289.csv
        home_home207.csv
        home_home176.csv
        home_home323.csv
        home_home166.csv
        home_home319.csv
        home_home82.csv
        home_home256.csv
        home_home311.csv
        home_home86.csv
        home_home123.csv
        home_home295.csv
        home_home271.csv
        home_home134.csv
        home_home143.csv
        home_home70.csv
        home_home74.csv
        home_home287.csv
        home_home148.csv
        home_home47.csv
        home_home170.csv
        home_home228.csv
        home_home88.csv
        home_home247.csv
        home_home263.csv
        home_home94.csv
        home_home283.csv
        home_home72.csv
        home_home327.csv
        home_home298.csv
        home_home243.csv
        home_home115.csv
        home_home202.csv
        home_home205.csv
        home_home150.csv
        home_home334.csv
        home_home241.csv
        home_home302.csv
        home_home267.csv
        home_home257.csv
        home_home102.csv
        home_home255.csv
        home_home252.csv
        home_home161.csv
        home_home122.csv
        home_home186.csv
        home_home157.csv
        home_home110.csv
        home_home270.csv
        home_home175.csv
        home_home140.csv
        home_home163.csv
        home_home185.csv
        home_home81.csv
        home_home189.csv
        home_home299.csv
        home_home167.csv
        home_home288.csv
        home_home236.csv
        home_home210.csv
        home_home304.csv
        home_home188.csv
        home_home333.csv
        home_home69.csv
        home_home328.csv
        home_home294.csv
        home_home195.csv
        home_home261.csv
        home_home121.csv
        home_home219.csv
        home_home215.csv
        home_home180.csv
        home_home280.csv
        home_home120.csv
        home_home105.csv
        home_home174.csv
        home_home284.csv
        home_home313.csv
        home_home244.csv
        home_home266.csv
        home_home330.csv
        home_home230.csv
        home_home154.csv
        home_home277.csv
        home_home309.csv
        home_home187.csv
        home_home325.csv
        home_home315.csv
        home_home272.csv
        home_home96.csv
        home_home151.csv
        home_home211.csv
        home_home259.csv
        home_home321.csv
        home_home225.csv
        home_home213.csv
        home_home118.csv
        home_home318.csv
        home_home199.csv
        home_home73.csv
        home_home296.csv
        home_home316.csv
        home_home234.csv
        home_home98.csv
        home_home89.csv
        home_home129.csv
        home_home194.csv
        home_home291.csv
        home_home335.csv
        home_home61.csv
        home_home326.csv
        home_home332.csv
        home_home113.csv
        home_home79.csv
        home_home253.csv
        home_home78.csv
        home_home320.csv
        home_home329.csv
        home_home223.csv
        home_home200.csv
        home_home240.csv
        home_home136.csv
        home_home300.csv
        home_home107.csv
        home_home173.csv
        home_home169.csv
        home_home144.csv
        home_home226.csv
        home_home133.csv
        home_home119.csv
        home_home262.csv
        home_home59.csv
        home_home152.csv
        home_home201.csv
        home_home137.csv
        home_home135.csv
        home_home126.csv
        home_home239.csv
        home_home238.csv
        home_home156.csv
        home_home245.csv
        home_home114.csv
        home_home308.csv
        home_home125.csv
        home_home301.csv
        home_home251.csv
        home_home224.csv
        home_home331.csv
        home_home218.csv
        home_home246.csv
        home_home67.csv
        home_home209.csv
        home_home97.csv
        home_home242.csv
        home_home208.csv
        home_home197.csv
        home_home290.csv
        home_home212.csv
        home_home109.csv
        home_home229.csv
        home_home285.csv
        home_home192.csv
        home_home83.csv
        home_home90.csv
        home_home62.csv
        home_home80.csv
        home_home237.csv
        home_home260.csv
        home_home141.csv
        home_home190.csv
        home_home164.csv
        home_home63.csv
        home_home227.csv
        home_home138.csv
        home_home99.csv
        home_home303.csv
        home_home248.csv
        home_home65.csv
        home_home178.csv
        home_home307.csv
        home_home93.csv
        home_home77.csv
        home_home231.csv
        home_home322.csv
        home_home68.csv
        home_home85.csv
        home_home184.csv
        home_home286.csv
        home_home183.csv
        home_home168.csv
        home_home273.csv
        home_home233.csv
        home_home75.csv
        home_home153.csv
        home_home116.csv
        home_home278.csv
        home_home179.csv
        home_home235.csv
        home_home84.csv
        home_home66.csv
        home_home282.csv
        home_home305.csv
        home_home158.csv
        home_home149.csv
        home_home281.csv
        home_home268.csv
        home_home165.csv
        home_home100.csv
        home_home275.csv
        home_home162.csv
        home_home221.csv
        home_home203.csv
        home_home214.csv
        home_home155.csv
        home_home171.csv
        home_home181.csv
        home_home310.csv
        home_home106.csv
        home_home177.csv
        home_home191.csv
        home_home293.csv
        home_home92.csv
        home_home276.csv
        home_home128.csv
        home_home254.csv
        home_home292.csv
        home_home258.csv
        home_home250.csv
        home_home64.csv
        home_home147.csv
        home_home145.csv
        home_home101.csv
        home_home146.csv
        home_home264.csv
        home_home71.csv
        home_home159.csv
        home_home193.csv
        home_home124.csv
        home_home317.csv
        home_home249.csv
        home_home222.csv
        home_home182.csv
        home_home279.csv
        home_home269.csv
        home_home91.csv
        home_home265.csv

##### FILE: scratchpad.py
==================================================
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


##### FILE: arima3.py
==================================================
import os
import random
import numpy as np
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
import pandas as pd

# ========================================
# Global Settings and Configuration
# ========================================

# Library settings
pd.set_option('display.width', 1000)
pd.set_option('display.max_columns', None)

# Directories
CHARTS_DIR = "./Marij/charts"
TEST_DATA_DIR = "./Marij/ind_homes"  # Directory for independent home data
os.makedirs(CHARTS_DIR, exist_ok=True)

def clear_chart_directory():
    """Clear all files in the charts directory."""
    for filename in os.listdir(CHARTS_DIR):
        file_path = os.path.join(CHARTS_DIR, filename)
        try:
            if os.path.isfile(file_path):
                os.remove(file_path)
                # print(f"Deleted: {file_path}")
        except Exception as e:
            print(f"Error deleting file {file_path}: {e}")

# ========================================
# Data Preprocessing Functions
# ========================================


def rename_columns(column_name):
    """Rename columns for simplicity."""
    parts = column_name.split('_')
    if len(parts) > 1:
        room = parts[1][:3]
        sensor_map = {
            'light': 'light',
            'humidity': 'humid',
            'temperature': 'temp',
            'hot-water-cold-pipe': 'coldpipe',
            'hot-water-hot-pipe': 'hotpipe',
            'central-heating-return': 'chreturn',
            'central-heating-flow': 'chflow',
            'electric-combined': 'elec',
            'gas': 'gas'
        }
        sensor = sensor_map.get(parts[-1], parts[-1])
        return f"{room}_{sensor}"
    return column_name


def preprocess_data(df):
    """
    Preprocess the dataset: rename columns, handle missing values,
    enforce time index frequency, and derive electricity and gas features.
    """
    # Rename columns for simplicity
    df.rename(columns={col: rename_columns(col)
              for col in df.columns}, inplace=True)

    # Ensure 'timestamp' is in datetime format
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)

    # Enforce a consistent frequency for the time index
    start_time = df.index.min()
    end_time = df.index.max()
    full_index = pd.date_range(
        start=start_time, end=end_time, freq='h')  # Hourly frequency

    # Reindex the DataFrame and fill missing timestamps with NaNs
    df = df.reindex(full_index)
    df.index.name = 'timestamp'

    # Impute missing values for key columns
    if 'hal_elec' in df.columns:
        df['hal_elec'] = df['hal_elec'].interpolate(
            method='linear').bfill().ffill()
    if 'hal_gas' in df.columns:
        df['hal_gas'] = df['hal_gas'].fillna(0)

    # Convert and sum electricity and gas data
    df['elec'] = df.get('hal_elec', 0) * 0.00027778  # Convert joules to Wh
    df['gas'] = df.get('hal_gas', 0)  # Already in Wh

    # Retain only relevant columns
    df = df[['elec', 'gas']].dropna()
    return df


# ========================================
# Time Series Analysis and Modeling
# ========================================


def fit_arima_and_save_charts(df, col, order, home_name):
    """
    Fit ARIMA model, save fitted vs actual and residual plots for the specified column.
    """
    # Fit ARIMA model
    model = ARIMA(df[col], order=order)
    fitted_model = model.fit()
    df['fitted'] = fitted_model.fittedvalues

    # Fitted vs Actual chart
    plt.figure(figsize=(10, 6))
    plt.plot(df.index, df[col], label='Actual', color='blue')
    plt.plot(df.index, df['fitted'], label='Fitted', color='orange')
    plt.title(
        f'{home_name} - {col.capitalize()} - Fitted vs Actual', fontsize=16)
    plt.legend()
    fitted_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_fitted_vs_actual.png")
    plt.savefig(fitted_chart_path)
    plt.close()
    # print(f"Saved chart: {fitted_chart_path}")

    # Residuals chart
    residuals = df[col] - df['fitted']
    plt.figure(figsize=(10, 6))
    plt.plot(residuals, label='Residuals', color='purple')
    plt.axhline(0, color='red', linestyle='--')
    plt.title(f'{home_name} - {col.capitalize()} - Residuals', fontsize=16)
    residuals_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_residuals.png")
    plt.savefig(residuals_chart_path)
    plt.close()
    # print(f"Saved chart: {residuals_chart_path}")

    return fitted_model


def check_stationarity(series, series_name):
    """Perform the Augmented Dickey-Fuller test for stationarity."""
    result = adfuller(series)
    print(f"ADF Statistic for {series_name}: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    if result[1] <= 0.05:
        print(f"The {series_name} series is stationary.")
    else:
        print(f"The {series_name} series is not stationary.")


# ========================================
# Evaluation and Cross-Validation
# ========================================

def evaluate_model(df, col, fitted_col):
    """Evaluate the model and calculate Normalized RMSE."""
    rmse = np.sqrt(mean_squared_error(df[col], df[fitted_col]))
    mean_val = df[col].mean()
    nrmse = rmse / mean_val
    return nrmse


def cross_validate_model(test_files, base_model_order, columns):
    """
    Perform cross-validation on a random subset of test files for specified columns.
    """
    results = {}
    selected_files = random.sample(test_files, 5)  # Randomly choose 5 files

    # Process each selected file
    for file in selected_files:
        # Use file name (e.g., home_home59) as home name
        home_name = file.split('.')[0]
        print(f"Processing test file: {file}")

        # Load and preprocess the file
        test_df = pd.read_csv(os.path.join(TEST_DATA_DIR, file))
        test_df = preprocess_data(test_df)
        test_df.index = pd.date_range(
            start=test_df.index.min(), periods=len(test_df), freq='h')

        # Store accuracy metrics for both columns (e.g., 'elec' and 'gas')
        home_results = {}
        for col in columns:
            print(f"Fitting ARIMA model for {col}...")
            fit_arima_and_save_charts(
                test_df, col, base_model_order, home_name)  # Save charts
            nrmse = evaluate_model(test_df, col, 'fitted')  # Evaluate model
            home_results[col] = nrmse

        results[home_name] = home_results
    return results


# ========================================
# Main Execution
# ========================================

def main():
    clear_chart_directory()
    data_file = './sensor_data_47.csv'
    base_model_order = (1, 0, 1)
    evaluation_columns = ['elec', 'gas']  # Columns to evaluate separately

    # Load and preprocess the primary dataset
    df = pd.read_csv(data_file)
    df = preprocess_data(df)

    # Stationarity check on electricity consumption
    print("Performing stationarity check on electricity...")
    check_stationarity(df['elec'], 'Electricity Consumption')

    # Fit and save ARIMA charts for primary dataset
    print("Fitting ARIMA for primary dataset...")
    for col in evaluation_columns:
        fitted_model = fit_arima_and_save_charts(df, col, base_model_order, 'Primary_Dataset')
        
        # Calculate and print RMSE for the primary dataset
        nrmse = evaluate_model(df, col, 'fitted')
        print(f"Normalized RMSE for {col.capitalize()} in Primary Dataset: {nrmse:.4f}")

    # Evaluate on random test homes
    print("Starting cross-validation...")
    test_files = os.listdir(TEST_DATA_DIR)
    cross_val_results = cross_validate_model(
        test_files, base_model_order, evaluation_columns)

    # Output cross-validation results
    print("Cross-validation results (Normalized RMSE per home per column):")
    for home, metrics in cross_val_results.items():
        print(f"{home}:")
        for col, nrmse in metrics.items():
            print(f"  {col.capitalize()}: {nrmse:.4f}")


def test():
    data_file = './Marij/ind_homes/home_home167.csv'
    base_model_order = (1, 0, 1)
    evaluation_columns = ['elec', 'gas']  # Columns to evaluate separately

    # Load and preprocess the primary dataset
    df = pd.read_csv(data_file)
    df = preprocess_data(df)
    df.set_index('timestamp', inplace=True)

    # Fit and save ARIMA charts for primary dataset
    print("Fitting ARIMA for primary dataset...")
    for col in evaluation_columns:
        fit_arima_and_save_charts(df, col, base_model_order, 'Primary_Dataset')


if __name__ == "__main__":
    main()


##### FILE: lstm1.py
==================================================
# TODO
# ARIMA hyperparameter tuning (or auto-tune)
# SARIMAX
# Fix cross-validation (cleaning of other files)
# Enable GPU/Cuda

import os
import random
import numpy as np
from sklearn.metrics import mean_squared_error
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.layers import Input
from sklearn.preprocessing import MinMaxScaler


# ========================================
# Global Settings and Configuration
# ========================================

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# Library settings
pd.set_option('display.width', 1000)
pd.set_option('display.max_columns', None)

# Directories
CHARTS_DIR = r"./Marij/charts"
TEST_DATA_DIR = r"./Marij/ind_homes"  # Directory for independent home data
os.makedirs(CHARTS_DIR, exist_ok=True)


def clear_chart_directory():
    """Clear all files in the charts directory."""
    for filename in os.listdir(CHARTS_DIR):
        file_path = os.path.join(CHARTS_DIR, filename)
        try:
            if os.path.isfile(file_path):
                os.remove(file_path)
                # print(f"Deleted: {file_path}")
        except Exception as e:
            print(f"Error deleting file {file_path}: {e}")

# ========================================
# Data Preprocessing Functions
# ========================================


def rename_columns(column_name):
    """Rename columns for simplicity."""
    parts = column_name.split('_')
    if len(parts) > 1:
        room = parts[1][:3]
        sensor_map = {
            'light': 'light',
            'humidity': 'humid',
            'temperature': 'temp',
            'hot-water-cold-pipe': 'coldpipe',
            'hot-water-hot-pipe': 'hotpipe',
            'central-heating-return': 'chreturn',
            'central-heating-flow': 'chflow',
            'electric-combined': 'elec',
            'gas': 'gas'
        }
        sensor = sensor_map.get(parts[-1], parts[-1])
        return f"{room}_{sensor}"
    return column_name


def preprocess_data(df):
    """
    Preprocess the dataset: rename columns, handle missing values,
    enforce time index frequency, and derive electricity and gas features.
    """
    # Rename columns for simplicity
    df.rename(columns={col: rename_columns(col)
              for col in df.columns}, inplace=True)

    # Ensure 'timestamp' is in datetime format
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df.set_index('timestamp', inplace=True)

    # Enforce a consistent frequency for the time index
    start_time = df.index.min()
    end_time = df.index.max()
    full_index = pd.date_range(
        start=start_time, end=end_time, freq='h')  # Hourly frequency

    # Reindex the DataFrame and fill missing timestamps with NaNs
    df = df.reindex(full_index)
    df.index.name = 'timestamp'

    # Impute missing values for key columns
    if 'hal_elec' in df.columns:
        df['hal_elec'] = df['hal_elec'].interpolate(
            method='linear').bfill().ffill()
    if 'hal_gas' in df.columns:
        df['hal_gas'] = df['hal_gas'].fillna(0)

    # Convert and sum electricity and gas data
    df['elec'] = df.get('hal_elec', 0) * 0.00027778  # Convert joules to Wh
    df['gas'] = df.get('hal_gas', 0)  # Already in Wh

    # Retain only relevant columns
    df = df[['elec', 'gas']].dropna()
    return df


# ========================================
# Time Series Analysis and Modeling
# ========================================


def fit_arima_and_save_charts(df, col, order, home_name):
    """
    Fit ARIMA model, save fitted vs actual and residual plots for the specified column.
    """
    # Fit ARIMA model
    model = ARIMA(df[col], order=order)
    fitted_model = model.fit()
    df['fitted'] = fitted_model.fittedvalues

    # Fitted vs Actual chart
    plt.figure(figsize=(10, 6))
    plt.plot(df.index, df[col], label='Actual', color='blue')
    plt.plot(df.index, df['fitted'], label='Fitted', color='orange')
    plt.title(
        f'{home_name} - {col.capitalize()} - Fitted vs Actual', fontsize=16)
    plt.legend()
    fitted_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_fitted_vs_actual.png")
    plt.savefig(fitted_chart_path)
    plt.close()
    # print(f"Saved chart: {fitted_chart_path}")

    # Residuals chart
    residuals = df[col] - df['fitted']
    plt.figure(figsize=(10, 6))
    plt.plot(residuals, label='Residuals', color='purple')
    plt.axhline(0, color='red', linestyle='--')
    plt.title(f'{home_name} - {col.capitalize()} - Residuals', fontsize=16)
    residuals_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_residuals.png")
    plt.savefig(residuals_chart_path)
    plt.close()
    # print(f"Saved chart: {residuals_chart_path}")

    return fitted_model


def check_stationarity(series, series_name):
    """Perform the Augmented Dickey-Fuller test for stationarity."""
    result = adfuller(series)
    print(f"ADF Statistic for {series_name}: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    if result[1] <= 0.05:
        print(f"The {series_name} series is stationary.")
    else:
        print(f"The {series_name} series is not stationary.")


def fit_lstm_and_save_charts(df, col, home_name):
    """
    Fit an LSTM model, save fitted vs actual and residual plots for the specified column.
    """
    # Scale the data
    scaler = MinMaxScaler(feature_range=(0, 1))
    scaled_data = scaler.fit_transform(df[col].values.reshape(-1, 1))

    # Prepare the data for LSTM
    sequence_length = 24  # Use past 24 hours to predict the next value
    X, y = [], []
    for i in range(sequence_length, len(scaled_data)):
        X.append(scaled_data[i-sequence_length:i, 0])
        y.append(scaled_data[i, 0])
    X, y = np.array(X), np.array(y)
    # [samples, timesteps, features]
    X = np.reshape(X, (X.shape[0], X.shape[1], 1))

    # Split into train and test sets
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    time_index = df.index[sequence_length:]  # Adjust index for the sequences

    # Build the LSTM model
    model = Sequential()
    model.add(Input(shape=(X.shape[1], 1)))
    model.add(LSTM(units=50, return_sequences=True))
    model.add(LSTM(units=50))
    model.add(Dense(1))

    model.compile(loss='mean_squared_error', optimizer='adam')

    # Train the model
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)

    # Make predictions
    predictions = model.predict(X)
    predictions = scaler.inverse_transform(predictions)

    # Prepare DataFrame for plotting
    df_pred = pd.DataFrame({
        'timestamp': time_index,
        'actual': df[col].values[sequence_length:],
        'predicted': predictions.flatten()
    }).set_index('timestamp')

    # Fitted vs Actual chart
    plt.figure(figsize=(10, 6))
    plt.plot(df_pred.index, df_pred['actual'], label='Actual', color='blue')
    plt.plot(df_pred.index, df_pred['predicted'],
             label='Predicted', color='green')
    plt.title(
        f'{home_name} - {col.capitalize()} - LSTM Fitted vs Actual', fontsize=16)
    plt.legend()
    fitted_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_lstm_fitted_vs_actual.png")
    plt.savefig(fitted_chart_path)
    plt.close()

    # Residuals chart
    residuals = df_pred['actual'] - df_pred['predicted']
    plt.figure(figsize=(10, 6))
    plt.plot(residuals.index, residuals, label='Residuals', color='purple')
    plt.axhline(0, color='red', linestyle='--')
    plt.title(f'{home_name} - {col.capitalize()} - LSTM Residuals', fontsize=16)
    residuals_chart_path = os.path.join(
        CHARTS_DIR, f"{home_name}_{col}_lstm_residuals.png")
    plt.savefig(residuals_chart_path)
    plt.close()

    # Calculate Normalized RMSE
    rmse = np.sqrt(mean_squared_error(df_pred['actual'], df_pred['predicted']))
    mean_val = df_pred['actual'].mean()
    nrmse = rmse / mean_val

    print(
        f"Normalized RMSE for {col.capitalize()} using LSTM in {home_name}: {nrmse:.4f}")


# ========================================
# Evaluation and Cross-Validation
# ========================================

def evaluate_model(df, col, fitted_col):
    """Evaluate the model and calculate Normalized RMSE."""
    rmse = np.sqrt(mean_squared_error(df[col], df[fitted_col]))
    mean_val = df[col].mean()
    nrmse = rmse / mean_val
    return nrmse


def cross_validate_model(test_files, base_model_order, columns):
    """
    Perform cross-validation on a random subset of test files for specified columns.
    """
    results = {}
    selected_files = random.sample(test_files, 5)  # Randomly choose 5 files

    # Process each selected file
    for file in selected_files:
        # Use file name (e.g., home_home59) as home name
        home_name = file.split('.')[0]
        print(f"Processing test file: {file}")

        # Load and preprocess the file
        test_df = pd.read_csv(os.path.join(TEST_DATA_DIR, file))
        test_df = preprocess_data(test_df)
        test_df.index = pd.date_range(
            start=test_df.index.min(), periods=len(test_df), freq='h')

        # Store accuracy metrics for both columns (e.g., 'elec' and 'gas')
        home_results = {}
        for col in columns:
            print(f"Fitting ARIMA model for {col}...")
            fit_arima_and_save_charts(
                test_df, col, base_model_order, home_name)  # Save charts
            nrmse = evaluate_model(test_df, col, 'fitted')  # Evaluate model
            home_results[col] = nrmse

        results[home_name] = home_results
    return results


# ========================================
# Main Execution
# ========================================

def main():
    clear_chart_directory()
    data_file = r'./sensor_data_47.csv'
    base_model_order = (1, 0, 1)
    evaluation_columns = ['elec', 'gas']  # Columns to evaluate separately

    # Load and preprocess the primary dataset
    df = pd.read_csv(data_file)
    df = preprocess_data(df)

    # Stationarity check on electricity consumption
    print("Performing stationarity check on electricity...")
    check_stationarity(df['elec'], 'Electricity Consumption')

    # Fit and save ARIMA charts for primary dataset
    print("Fitting ARIMA for primary dataset...")
    for col in evaluation_columns:
        fitted_model = fit_arima_and_save_charts(
            df, col, base_model_order, 'Primary_Dataset')

        # Calculate and print NRMSE for the primary dataset using ARIMA
        nrmse_arima = evaluate_model(df, col, 'fitted')
        print(
            f"Normalized RMSE for {col.capitalize()} using ARIMA in Primary Dataset: {nrmse_arima:.4f}")

    # Fit and save LSTM charts for primary dataset
    print("Fitting LSTM for primary dataset...")
    for col in evaluation_columns:
        fit_lstm_and_save_charts(df, col, 'Primary_Dataset')

    # Cross-validation code can be commented out for now
    print("Starting cross-validation...")
    test_files = os.listdir(TEST_DATA_DIR)
    cross_val_results = cross_validate_model(
        test_files, base_model_order, evaluation_columns)

    # Output cross-validation results
    print("Cross-validation results (Normalized RMSE per home per column):")
    for home, metrics in cross_val_results.items():
        print(f"{home}:")
        for col, nrmse in metrics.items():
            print(f"  {col.capitalize()}: {nrmse:.4f}")


def test():
    data_file = r'./Marij/ind_homes/home_home167.csv'
    base_model_order = (1, 0, 1)
    evaluation_columns = ['elec', 'gas']  # Columns to evaluate separately

    # Load and preprocess the primary dataset
    df = pd.read_csv(data_file)
    df = preprocess_data(df)
    df.set_index('timestamp', inplace=True)

    # Fit and save ARIMA charts for primary dataset
    print("Fitting ARIMA for primary dataset...")
    for col in evaluation_columns:
        fit_arima_and_save_charts(df, col, base_model_order, 'Primary_Dataset')


if __name__ == "__main__":
    main()


##### FILE: Archive/household_data_v1.py
==================================================
import os
import pandas as pd

# Path to the folder containing the sensor data files
folder_path = './raw data/sensordata/'

# Initialize counters and file structure samples for each category
categories = {
    'electric-mains': 0,
    'gas-pulse': 0,
    'tempprobe': 0,
    'electric-subcircuit': 0,
    'unlabelled-subcircuit': 0,
}

# Dictionary to store a sample structure of each category
file_structures = {key: None for key in categories}

# Iterate through all files in the folder
for file_name in os.listdir(folder_path):
    if 'electric-mains' in file_name:
        categories['electric-mains'] += 1
        if file_structures['electric-mains'] is None:
            file_structures['electric-mains'] = pd.read_csv(os.path.join(folder_path, file_name)).head()
    elif 'gas-pulse' in file_name:
        categories['gas-pulse'] += 1
        if file_structures['gas-pulse'] is None:
            file_structures['gas-pulse'] = pd.read_csv(os.path.join(folder_path, file_name)).head()
    elif 'tempprobe' in file_name:
        categories['tempprobe'] += 1
        if file_structures['tempprobe'] is None:
            file_structures['tempprobe'] = pd.read_csv(os.path.join(folder_path, file_name)).head()
    elif 'electric-subcircuit' in file_name:
        categories['electric-subcircuit'] += 1
        if file_structures['electric-subcircuit'] is None:
            file_structures['electric-subcircuit'] = pd.read_csv(os.path.join(folder_path, file_name)).head()
    elif 'unlabelled' in file_name:
        categories['unlabelled-subcircuit'] += 1
        if file_structures['unlabelled-subcircuit'] is None:
            file_structures['unlabelled-subcircuit'] = pd.read_csv(os.path.join(folder_path, file_name)).head()

# Output file counts for each category
print("File counts per category:")
for category, count in categories.items():
    print(f"{category}: {count}")

# Display a sample structure of each category for review
print("\nSample structure of files:")
for category, df in file_structures.items():
    print(f"\n{category} sample:")
    print(df)


##### FILE: Archive/household_data_v2.py
==================================================
import os
import pandas as pd
import re

# Specify your folder path
folder_path = './raw data/sensordata/'


def clean_name(name):
    """
    This function removes trailing sensor numbers, variations like 'sensorXXXXc', and specific room IDs,
    keeping only the core 'sensor' or room name.
    """
    # Clean room numbers (e.g., hall654 -> hall)
    name = re.sub(r'\d+$', '', name)
    # Normalize all sensor variations 'sensorXXX', 'sensorXXXc', 'sensorcXXX' to just 'sensor'
    name = re.sub(r'sensorc?\d*c?', 'sensor', name)
    return name


def get_files_info(folder_path):
    # List all files in folder (only CSVs)
    file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

    # Total number of CSV files
    total_files = len(file_list)

    # Extract room and sensor information
    room_sensor_data = []
    for file_name in file_list:
        parts = file_name.split('_')
        if len(parts) >= 3:
            room = clean_name(parts[1])  # Clean room name by removing trailing digits
            sensor = clean_name(parts[2])  # Clean sensor name and normalize variations
            room_sensor_data.append((room, sensor))

    # Create a dataframe for analysis
    df = pd.DataFrame(room_sensor_data, columns=['Room', 'Sensor'])

    return total_files, df


# Call the function to retrieve file info
total_files, df = get_files_info(folder_path)

# Get unique rooms (after cleaning)
unique_rooms = df['Room'].unique()

# Aggregating data for sensor types and room occurrences
sensor_summary = df.groupby(['Sensor', 'Room']).size().reset_index(name='Count')

# Print results
print(f"Total number of CSV files: {total_files}")
print("\nList of Unique Rooms:")
print(unique_rooms)

print("\nTable Summary (Sensor Type, Room, Count):")
print(sensor_summary.to_string(index=False))


##### FILE: Archive/room_summary.py
==================================================
import pandas as pd
import re

# Read the summary.csv file
summary_df = pd.read_csv('summary.csv')

# Exclude non-sensor columns
sensor_columns = summary_df.columns.difference(
    ['home_number', 'earliest_timestamp', 'latest_timestamp', 'number_of_rows'])

# Initialize a nested dictionary to hold counts
# Structure: {sensor_type: {room: count, ...}, ...}
sensor_room_counts = {}

# Process each sensor column to extract room and sensor type
for col in sensor_columns:
    # Clean the column name by removing numeric suffixes and 'sensor' word
    col_cleaned = re.sub(r'\d+', '', col)
    col_cleaned = col_cleaned.replace('sensor', '')
    parts = col_cleaned.split('_')

    if len(parts) >= 2:
        room = parts[0]
        sensor_type = '_'.join(parts[1:])
    else:
        continue  # Skip columns that don't match the expected pattern

    # Ensure the sensor_type exists in the dictionary
    if sensor_type not in sensor_room_counts:
        sensor_room_counts[sensor_type] = {}
    # Initialize the room type count if not present
    if room not in sensor_room_counts[sensor_type]:
        sensor_room_counts[sensor_type][room] = 0

    # Count the number of homes where the number of valid datapoints is greater than 0
    count_homes = (summary_df[col] > 0).sum()
    sensor_room_counts[sensor_type][room] += count_homes

# Convert the nested dictionary to a DataFrame
sensor_summary_df = pd.DataFrame.from_dict(sensor_room_counts, orient='index').fillna(0).astype(int)

# Optionally, sort the rows and columns for better readability
sensor_summary_df = sensor_summary_df.sort_index()
sensor_summary_df = sensor_summary_df.reindex(sorted(sensor_summary_df.columns), axis=1)

# Save the DataFrame to a CSV file
sensor_summary_df.to_csv('room_summary.csv')

print("Room summary CSV (with sensors as rows and rooms as columns) has been generated as 'room_summary.csv'.")


##### FILE: Archive/room_summary2.py
==================================================
import pandas as pd
import re

# Read the summary.csv file
summary_df = pd.read_csv('summary.csv')

# Exclude non-sensor columns
sensor_columns = summary_df.columns.difference(
    ['home_number', 'earliest_timestamp', 'latest_timestamp', 'number_of_rows'])

# Initialize a nested dictionary to hold counts
sensor_room_counts = {}
sensor_stats = {}

# Process each sensor column to extract room and sensor type
for col in sensor_columns:
    # Clean the column name by removing numeric suffixes and 'sensor' word
    col_cleaned = re.sub(r'\d+', '', col)
    col_cleaned = col_cleaned.replace('sensor', '')
    parts = col_cleaned.split('_')

    if len(parts) >= 2:
        room = parts[0]
        sensor_type = '_'.join(parts[1:])
    else:
        continue  # Skip columns that don't match the expected pattern

    # Ensure the sensor_type exists in the dictionary for room counts
    if sensor_type not in sensor_room_counts:
        sensor_room_counts[sensor_type] = {}
    if room not in sensor_room_counts[sensor_type]:
        sensor_room_counts[sensor_type][room] = 0

    # Count the number of homes where the number of valid datapoints is greater than 0
    count_homes = (summary_df[col] > 0).sum()
    sensor_room_counts[sensor_type][room] += count_homes

    # Calculate min/max statistics across all homes for the sensor
    min_value = summary_df[col].min()
    max_value = summary_df[col].max()

    # Infer units based on sensor type
    if 'temperature' in sensor_type:
        unit = '°C'
    elif 'humidity' in sensor_type:
        unit = '%'
    elif 'electric' in sensor_type or 'power' in sensor_type:
        unit = 'W'
    elif 'gas' in sensor_type:
        unit = 'Wh'
    elif 'light' in sensor_type:
        unit = 'Undetermined'
    else:
        unit = 'Unknown'

    # Store sensor statistics
    if sensor_type not in sensor_stats:
        sensor_stats[sensor_type] = {'min': min_value, 'max': max_value, 'unit': unit}

# Convert the nested dictionary for room counts to a DataFrame
sensor_summary_df = pd.DataFrame.from_dict(sensor_room_counts, orient='index').fillna(0).astype(int)

# Sort the rows and columns for better readability
sensor_summary_df = sensor_summary_df.sort_index()
sensor_summary_df = sensor_summary_df.reindex(sorted(sensor_summary_df.columns), axis=1)

# Save the room summary to CSV
sensor_summary_df.to_csv('room_summary2.csv')

# Convert sensor stats to DataFrame and save to CSV
sensor_stats_df = pd.DataFrame(sensor_stats).T
sensor_stats_df.to_csv('sensor_stats_summary.csv')

print("Room summary CSV has been generated as 'room_summary.csv'.")
print("Sensor stats summary CSV has been generated as 'sensor_stats_summary.csv'.")


##### FILE: Archive/merge2.py
==================================================
import os
import pandas as pd

directory = r"./raw data/auxiliarydata/hourly_readings/"

# Function to extract components (home, room, sensor, probe) from filenames
def parse_filename(filename):
    # Assumes filename format: home<number>_<room><number>_sensor<number>_<probe>.csv
    parts = filename.replace('.csv', '').split('_')
    home = parts[0]
    room = parts[1]
    sensor = parts[2]
    probe = "_".join(parts[3:])
    return home, room, sensor, probe

# Initialize an empty merged DataFrame
merged_df = pd.DataFrame()

# Iterate over all CSV files in the directory
for i, file in enumerate(os.listdir(directory)):
    if file.endswith(".csv"):
        file_path = os.path.join(directory, file)

        # Read csv
        df = pd.read_csv(file_path, header=None, names=['timestamp', 'value'])

        # Convert timestamp into datetime format
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

        # Extract metadata from the filename to create unique column names
        home, room, sensor, probe = parse_filename(file)
        sensor_column_name = f'{home}_{room}_{sensor}_{probe}'

        # Add the sensor data as a new column with the unique sensor name
        df[sensor_column_name] = df['value']
        df.drop(columns='value', inplace=True)

        # Merge the current DataFrame with the main merged dataframe, aligning by timestamp
        if merged_df.empty:
            # The first file determines the initial structure of the DataFrame
            merged_df = df
        else:
            # Incrementally merge with how='outer' to include all timestamps
            merged_df = pd.merge(merged_df, df, on='timestamp', how='outer')

        # For progress feedback
        if i % 100 == 0:
            print(f"Processed {i} files...")

# Sort the DataFrame by the timestamp after merging
merged_df.sort_values(by="timestamp", inplace=True)

merged_df.to_csv('merged_sensor_data.csv', index=False)

print("Merging complete. The file 'merged_sensor_data.csv' is ready.")


##### FILE: Archive/sensor_stats_2.py
==================================================
import pandas as pd
import re

input_file = 'sensor_stats.csv'
output_file = 'sensor_summary_combined.csv'

# Function to extract general sensor type from the full sensor type string
def extract_sensor_type(full_sensor_type):
    parts = full_sensor_type.split('_')
    return '_'.join(parts[1:])  # Ignore the room part (first element)

# Read the input CSV file
df = pd.read_csv(input_file)

# Extract sensor types without room prefixes
df['clean_sensor_type'] = df['sensor_type'].apply(extract_sensor_type)

# Define correct units for each sensor type
sensor_units = {
    'temperature': '°C',
    'humidity': '% RH',
    'light': 'Uncalibrated units',
    'electric-combined': 'Watts',
    'electric': 'Watts',
    'electric_appliance': 'Watts',
    'gas-pulse_gas': 'Watt hours',
    'power': 'Watts',
    'clamp_temperature': '°C',
    'tempprobe': '°C',
    'room_humidity': '% RH',
    'room_temperature': '°C',
    'room_light': 'Uncalibrated units',
    'heater_temperature': '°C',
    'heater_humidity': '% RH',
    # Add more sensor types and their units as needed
}

# Function to infer unit from sensor type
def infer_unit(sensor_type):
    for key in sensor_units:
        if key in sensor_type:
            return sensor_units[key]
    return 'Unknown'

# Group by sensor type, ignoring room prefixes, and calculate combined stats
summary = df.groupby('clean_sensor_type').agg(
    min_value=('min_value', 'min'),
    max_value=('max_value', 'max'),
    sum_value=('average_value', 'sum'),
    count=('average_value', 'count')  # To calculate global average
).reset_index()

# Calculate average values
summary['average_value'] = summary['sum_value'] / summary['count']

# Infer units based on sensor type
summary['unit'] = summary['clean_sensor_type'].apply(infer_unit)

# Drop the sum and count columns as they are no longer needed
summary.drop(columns=['sum_value', 'count'], inplace=True)

# Rename the columns for clarity
summary.rename(columns={'clean_sensor_type': 'sensor_type'}, inplace=True)

# Save the summary to a new CSV file
summary.to_csv(output_file, index=False)

print(f'Summary has been saved to {output_file}.')


##### FILE: Archive/extract_csv.py
==================================================
import os
import shutil

def copy_files_to_outer_folder():
    base_dir = r"./raw data/auxiliarydata/hourly_readings"

    # Check if the base directory exists
    if not os.path.exists(base_dir):
        print(f"Directory {base_dir} does not exist.")
        return

    # Go through all subfolders inside the base directory
    for root, dirs, files in os.walk(base_dir):
        # Skip the base directory itself (we only need the subfolders)
        if root == base_dir:
            continue

        for file_name in files:
            # Get full file path
            file_path = os.path.join(root, file_name)

            # Define the destination path (which is the base directory itself)
            dest_path = os.path.join(base_dir, file_name)

            # Copy the file to the base directory
            try:
                print(f"Copying {file_name} from {root} to {base_dir}")
                shutil.copy(file_path, dest_path)
            except shutil.SameFileError:
                print(f"{file_name} already exists in {base_dir}, skipping...")
            except Exception as e:
                print(f"Error copying {file_name}: {e}")

copy_files_to_outer_folder()


##### FILE: Archive/merge1.py
==================================================
import os
import pandas as pd

directory = r"./raw data/auxiliarydata/hourly_readings/"

# Check length of files
csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]

for file in csv_files[:]:
    file_path = os.path.join(directory, file)
    data = pd.read_csv(file_path, header=None)
    print(f"{file} - Number of rows: {len(data)}")


##### FILE: Archive/summary2.py
==================================================
import os
import pandas as pd
import re

directory = r"./ind_homes/"  # Directory where the homeXXX.csv files are located

# Function to extract general sensor type from column names
def parse_sensor_type(column_name):
    # Remove home prefix and split by underscores
    parts = column_name.split('_')
    # Ignore the home number ('homeXX') in the first part
    if parts[0].startswith('home'):
        parts = parts[1:]
    # Remove numerical suffixes and 'sensor' word
    cleaned_parts = []
    for part in parts:
        # Remove numeric characters
        part = re.sub(r'\d+', '', part)
        # Remove 'sensor' word
        part = part.replace('sensor', '')
        # Only add non-empty parts
        if part:
            cleaned_parts.append(part)
    # Rejoin the parts to form the general sensor type
    sensor_type = '_'.join(cleaned_parts)
    return sensor_type

# Initialize a set to collect all sensor types
all_sensor_types = set()

# Collect all home CSV files
home_files = [f for f in os.listdir(directory) if f.startswith('home') and f.endswith('.csv')]

# First pass to collect all sensor types
for file in home_files:
    file_path = os.path.join(directory, file)
    df = pd.read_csv(file_path)
    # Exclude 'timestamp' column
    sensor_columns = [col for col in df.columns if col != 'timestamp']
    for col in sensor_columns:
        sensor_type = parse_sensor_type(col)
        all_sensor_types.add(sensor_type)

# Convert set to sorted list for consistent column order
all_sensor_types = sorted(all_sensor_types)

# Initialize list to hold summary data
summary_data = []

# Second pass to compute summary per home
for file in home_files:
    file_path = os.path.join(directory, file)
    df = pd.read_csv(file_path)
    # Ensure 'timestamp' column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    home_number_match = re.search(r'home(\d+).csv', file)
    if home_number_match:
        home_number = home_number_match.group(1)
    else:
        home_number = file.replace('home', '').replace('.csv', '')
    earliest_timestamp = df['timestamp'].min()
    latest_timestamp = df['timestamp'].max()
    num_rows = len(df)

    # Initialize dict for sensor counts
    sensor_counts = dict.fromkeys(all_sensor_types, 0)

    # Count non-null data points for each sensor type
    for col in df.columns:
        if col != 'timestamp':
            sensor_type = parse_sensor_type(col)
            count_non_null = df[col].count()
            sensor_counts[sensor_type] += count_non_null

    # Prepare the row data
    row_data = {
        'home_number': home_number,
        'earliest_timestamp': earliest_timestamp,
        'latest_timestamp': latest_timestamp,
        'number_of_rows': num_rows
    }
    # Add sensor counts to row data
    row_data.update(sensor_counts)
    # Append to summary data list
    summary_data.append(row_data)

# Create DataFrame for summary
summary_df = pd.DataFrame(summary_data)

# Save summary DataFrame to CSV
summary_df.to_csv('summary2.csv', index=False)

# Function to compute min, max, and average values for each sensor type across all homes
def compute_sensor_stats(directory, all_sensor_types):
    sensor_stats = {sensor: {'min': float('inf'), 'max': float('-inf'), 'sum': 0.0, 'count': 0} for sensor in all_sensor_types}
    for file in os.listdir(directory):
        if file.startswith('home') and file.endswith('.csv'):
            file_path = os.path.join(directory, file)
            df = pd.read_csv(file_path)
            for col in df.columns:
                if col != 'timestamp':
                    sensor_type = parse_sensor_type(col)
                    if sensor_type in sensor_stats:
                        column_data = df[col].dropna()
                        if not column_data.empty:
                            current_min = column_data.min()
                            current_max = column_data.max()
                            sensor_stats[sensor_type]['min'] = min(sensor_stats[sensor_type]['min'], current_min)
                            sensor_stats[sensor_type]['max'] = max(sensor_stats[sensor_type]['max'], current_max)
                            sensor_stats[sensor_type]['sum'] += column_data.sum()
                            sensor_stats[sensor_type]['count'] += column_data.count()

    # Infer units based on sensor type
    sensor_units = {
        'temperature': '°C',
        'humidity': '% RH',
        'light': 'Uncalibrated units',
        'electric_combined': 'Watts',
        'electric_mains': 'Watts',
        'electric_subcircuit': 'Watts',
        'electric_appliance': 'Watts',
        'electric': 'Watts',
        'gas_pulse': 'Watt hours',
        'gas': 'Watt hours',
        'power': 'Watts',
        'clamp_temperature': '°C',
        # Add more sensor types and their units as needed
    }

    # Prepare summary data
    summary_list = []
    for sensor, values in sensor_stats.items():
        unit = sensor_units.get(sensor, 'Unknown')
        min_val = values['min'] if values['min'] != float('inf') else None
        max_val = values['max'] if values['max'] != float('-inf') else None
        average_val = values['sum'] / values['count'] if values['count'] > 0 else None
        summary_list.append({
            'sensor_type': sensor,
            'min_value': min_val,
            'max_value': max_val,
            'average_value': average_val,
            'unit': unit
        })

    # Create DataFrame and save to CSV
    stats_df = pd.DataFrame(summary_list)
    stats_df.to_csv('sensor_stats.csv', index=False)
    print("Sensor stats CSV has been generated as 'sensor_stats.csv'.")

# Call the function to compute min, max, and average values
compute_sensor_stats(directory, all_sensor_types)

print("Summary CSV has been generated as 'summary.csv'.")


##### FILE: Archive/summary1.py
==================================================
import os
import pandas as pd
import re

directory = r"./ind_homes/"  # Directory where the homeXXX.csv files are located

# Function to extract general sensor type from column names
def parse_sensor_type(column_name):
    # Remove home prefix and split by underscores
    parts = column_name.split('_')
    # Ignore the home number ('homeXX') in the first part
    if parts[0].startswith('home'):
        parts = parts[1:]
    # Remove numerical suffixes and 'sensor' word
    cleaned_parts = []
    for part in parts:
        # Remove numeric characters
        part = re.sub(r'\d+', '', part)
        # Remove 'sensor' word
        part = part.replace('sensor', '')
        # Only add non-empty parts
        if part:
            cleaned_parts.append(part)
    # Rejoin the parts to form the general sensor type
    sensor_type = '_'.join(cleaned_parts)
    return sensor_type

# Initialize a set to collect all sensor types
all_sensor_types = set()

# Collect all home CSV files
home_files = [f for f in os.listdir(directory) if f.startswith('home') and f.endswith('.csv')]

# First pass to collect all sensor types
for file in home_files:
    file_path = os.path.join(directory, file)
    df = pd.read_csv(file_path)
    # Exclude 'timestamp' column
    sensor_columns = [col for col in df.columns if col != 'timestamp']
    for col in sensor_columns:
        sensor_type = parse_sensor_type(col)
        all_sensor_types.add(sensor_type)

# Convert set to sorted list for consistent column order
all_sensor_types = sorted(all_sensor_types)

# Initialize list to hold summary data
summary_data = []

# Second pass to compute summary per home
for file in home_files:
    file_path = os.path.join(directory, file)
    df = pd.read_csv(file_path)
    # Ensure 'timestamp' column is in datetime format
    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
    home_number_match = re.search(r'home_(\w+).csv', file)
    if home_number_match:
        home_number = home_number_match.group(1)
    else:
        home_number = file.replace('home', '').replace('.csv', '')
    earliest_timestamp = df['timestamp'].min()
    latest_timestamp = df['timestamp'].max()
    num_rows = len(df)

    # Initialize dict for sensor counts
    sensor_counts = dict.fromkeys(all_sensor_types, 0)

    # Count non-null data points for each sensor type
    for col in df.columns:
        if col != 'timestamp':
            sensor_type = parse_sensor_type(col)
            count_non_null = df[col].count()
            sensor_counts[sensor_type] += count_non_null

    # Prepare the row data
    row_data = {
        'home_number': home_number,
        'earliest_timestamp': earliest_timestamp,
        'latest_timestamp': latest_timestamp,
        'number_of_rows': num_rows
    }
    # Add sensor counts to row data
    row_data.update(sensor_counts)
    # Append to summary data list
    summary_data.append(row_data)

# Create DataFrame for summary
summary_df = pd.DataFrame(summary_data)

# Save summary DataFrame to CSV
summary_df.to_csv('summary.csv', index=False)

print("Summary CSV has been generated as 'summary.csv'.")


##### FILE: Archive/merge3.py
==================================================
import os
import pandas as pd

directory = r"./raw data/auxiliarydata/hourly_readings/"

# Function to extract components (home, room, sensor, probe) from filenames
def parse_filename(filename):
    # Assumes filename format: home<number>_<room><number>_sensor<number>_<probe>.csv
    parts = filename.replace('.csv', '').split('_')
    home = parts[0]
    room = parts[1]
    sensor = parts[2]
    probe = "_".join(parts[3:])
    return home, room, sensor, probe

# Initialize a dictionary to hold DataFrames for each home
home_dfs = {}

# Iterate over all CSV files in the directory
for i, file in enumerate(os.listdir(directory)):
    if file.endswith(".csv"):
        file_path = os.path.join(directory, file)

        # Read the CSV
        df = pd.read_csv(file_path, header=None, names=['timestamp', 'value'])

        # Convert the timestamp into datetime format
        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')

        # Extract metadata from the filename to create unique column names
        home, room, sensor, probe = parse_filename(file)
        sensor_column_name = f'{home}_{room}_{sensor}_{probe}'

        # Add the sensor data as a new column with the unique sensor name
        df[sensor_column_name] = df['value']
        df.drop(columns='value', inplace=True)

        # Check if this home already has a DataFrame, if not, initialize one
        if home not in home_dfs:
            home_dfs[home] = df  # First file for this home
        else:
            # Incrementally merge with how='outer' to include all timestamps
            home_dfs[home] = pd.merge(home_dfs[home], df, on='timestamp', how='outer')

        # For progress feedback
        if i % 100 == 0:
            print(f"Processed {i} files...")

# Save each home's DataFrame to a separate CSV file
for home, home_df in home_dfs.items():
    # Sort the DataFrame by timestamp
    home_df.sort_values(by="timestamp", inplace=True)

    # Define the output file name
    output_file_path = f"./home_{home}.csv"

    # Save the DataFrame to CSV
    home_df.to_csv(output_file_path, index=False)

    print(f"Saved {output_file_path}")

print("Processing complete. Separate files for each home have been generated.")


