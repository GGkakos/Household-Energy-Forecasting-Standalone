{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1 - Setup and Dataset Preparation\n",
    "#### 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marij/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 03:30:50.175953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735443050.195402   14394 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735443050.201456   14394 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-29 03:30:50.221220: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (/usr/lib/python3/dist-packages/google/protobuf/internal/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marima\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARIMA\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatespace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Input\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m typing \u001b[38;5;28;01mas\u001b[39;00m npt\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m message\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m full_type_pb2\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n",
      "File \u001b[0;32m~/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/lib/python3.10/site-packages/tensorflow/core/framework/attr_value_pb2.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# source: tensorflow/core/framework/attr_value.proto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (/usr/lib/python3/dist-packages/google/protobuf/internal/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Libraries for Data Manipulation, Pre-Processing and Visualisations\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Libraries for Fourier and KNN \n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Libraries for ML and DL models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from prophet import Prophet\n",
    "\n",
    "# Libraries for statistical \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Libraries for handling logs\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marij/GDrive/M1-Projects/9-Group-Projects/ML-project/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Tensorflow on GPU and Log Settings\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Print GPUs recognised by tensorflow\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Run tensorflow on GPU\n",
    "tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')  \n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Supress Tensorflow warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR) # Further supressing of tensorflow logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Processing and Saving Sensor Data by Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"./raw_data/hourly_readings/\"\n",
    "\n",
    "# Function to extract components (home, room, sensor, probe) from filenames\n",
    "def parse_filename(filename):\n",
    "    # Assumes filename format: home<number>_<room><number>_sensor<number>_<probe>.csv\n",
    "    parts = filename.replace('.csv.gz', '').replace('.csv', '').split('_')\n",
    "    home = parts[0]\n",
    "    room = parts[1]\n",
    "    sensor = parts[2]\n",
    "    probe = \"_\".join(parts[3:])\n",
    "    return home, room, sensor, probe\n",
    "\n",
    "# Initialize a dictionary to hold DataFrames for each home\n",
    "home_dfs = {}\n",
    "\n",
    "# Iterate over all CSV files in the directory\n",
    "for i, file in enumerate(os.listdir(directory)):\n",
    "\n",
    "    if file.endswith(\".csv\") or file.endswith('.csv.gz'):\n",
    "        file_path = os.path.join(directory, file)\n",
    "\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(file_path, header=None, names=['timestamp', 'value'])\n",
    "\n",
    "        # Convert the timestamp into datetime format\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "        # Extract metadata from the filename to create unique column names\n",
    "        home, room, sensor, probe = parse_filename(file)\n",
    "        sensor_column_name = f'{home}_{room}_{sensor}_{probe}'\n",
    "\n",
    "        # Add the sensor data as a new column with the unique sensor name\n",
    "        df[sensor_column_name] = df['value']\n",
    "        df.drop(columns='value', inplace=True)\n",
    "\n",
    "        # Check if this home already has a DataFrame, if not, initialize one\n",
    "        if home not in home_dfs:\n",
    "            home_dfs[home] = df  # First file for this home\n",
    "        else:\n",
    "            # Incrementally merge with how='outer' to include all timestamps\n",
    "            home_dfs[home] = pd.merge(home_dfs[home], df, on='timestamp', how='outer')\n",
    "\n",
    "        # For progress feedback\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i} files...\")\n",
    "\n",
    "# Save each home's DataFrame to a separate CSV file\n",
    "for home, home_df in home_dfs.items():\n",
    "    # Sort the DataFrame by timestamp\n",
    "    home_df.sort_values(by=\"timestamp\", inplace=True)\n",
    "\n",
    "    # Define the output file name\n",
    "    output_file_path = os.path.join(r\"./ind-homes\", f\"home_{home}.csv\")\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    home_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Saved {output_file_path}\")\n",
    "\n",
    "print(\"Processing complete. Separate files for each home have been generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 - Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generating a Summary of Sensor Data Across Homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"./ind-homes/\"  # Directory where the homeXXX.csv files are located\n",
    "\n",
    "# Function to extract general sensor type from column names\n",
    "def parse_sensor_type(column_name):\n",
    "    # Remove home prefix and split by underscores\n",
    "    parts = column_name.split('_')\n",
    "    # Ignore the home number ('homeXX') in the first part\n",
    "    if parts[0].startswith('home'):\n",
    "        parts = parts[1:]\n",
    "    # Remove numerical suffixes and 'sensor' word\n",
    "    cleaned_parts = []\n",
    "    for part in parts:\n",
    "        # Remove numeric characters\n",
    "        part = re.sub(r'\\d+', '', part)\n",
    "        # Remove 'sensor' word\n",
    "        part = part.replace('sensor', '')\n",
    "        # Only add non-empty parts\n",
    "        if part:\n",
    "            cleaned_parts.append(part)\n",
    "    # Rejoin the parts to form the general sensor type\n",
    "    sensor_type = '_'.join(cleaned_parts)\n",
    "    return sensor_type\n",
    "\n",
    "# Initialize a set to collect all sensor types\n",
    "all_sensor_types = set()\n",
    "\n",
    "# Collect all home CSV files\n",
    "home_files = [f for f in os.listdir(directory) if f.startswith('home') and f.endswith('.csv')]\n",
    "\n",
    "# First pass to collect all sensor types\n",
    "for file in home_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Exclude 'timestamp' column\n",
    "    sensor_columns = [col for col in df.columns if col != 'timestamp']\n",
    "    for col in sensor_columns:\n",
    "        sensor_type = parse_sensor_type(col)\n",
    "        all_sensor_types.add(sensor_type)\n",
    "\n",
    "# Convert set to sorted list for consistent column order\n",
    "all_sensor_types = sorted(all_sensor_types)\n",
    "\n",
    "# Initialize list to hold summary data\n",
    "summary_data = []\n",
    "\n",
    "# Second pass to compute summary per home\n",
    "for file in home_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Ensure 'timestamp' column is in datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    home_number_match = re.search(r'home(\\d+).csv', file)\n",
    "    if home_number_match:\n",
    "        home_number = home_number_match.group(1)\n",
    "    else:\n",
    "        home_number = file.replace('home', '').replace('.csv', '')\n",
    "    earliest_timestamp = df['timestamp'].min()\n",
    "    latest_timestamp = df['timestamp'].max()\n",
    "    num_rows = len(df)\n",
    "\n",
    "    # Initialize dict for sensor counts\n",
    "    sensor_counts = dict.fromkeys(all_sensor_types, 0)\n",
    "\n",
    "    # Count non-null data points for each sensor type\n",
    "    for col in df.columns:\n",
    "        if col != 'timestamp':\n",
    "            sensor_type = parse_sensor_type(col)\n",
    "            count_non_null = df[col].count()\n",
    "            sensor_counts[sensor_type] += count_non_null\n",
    "\n",
    "    # Prepare the row data\n",
    "    row_data = {\n",
    "        'home_number': home_number,\n",
    "        'earliest_timestamp': earliest_timestamp,\n",
    "        'latest_timestamp': latest_timestamp,\n",
    "        'number_of_rows': num_rows\n",
    "    }\n",
    "    # Add sensor counts to row data\n",
    "    row_data.update(sensor_counts)\n",
    "    # Append to summary data list\n",
    "    summary_data.append(row_data)\n",
    "\n",
    "# Create DataFrame for summary\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(summary_df)\n",
    "# Save summary DataFrame to CSV\n",
    "# summary_df.to_csv('summary2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Computing Sensor Statistics Across Homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute min, max, and average values for each sensor type across all homes\n",
    "def compute_sensor_stats(directory, all_sensor_types):\n",
    "    sensor_stats = {sensor: {'min': float('inf'), 'max': float('-inf'), 'sum': 0.0, 'count': 0} for sensor in all_sensor_types}\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith('home') and file.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            for col in df.columns:\n",
    "                if col != 'timestamp':\n",
    "                    sensor_type = parse_sensor_type(col)\n",
    "                    if sensor_type in sensor_stats:\n",
    "                        column_data = df[col].dropna()\n",
    "                        if not column_data.empty:\n",
    "                            current_min = column_data.min()\n",
    "                            current_max = column_data.max()\n",
    "                            sensor_stats[sensor_type]['min'] = min(sensor_stats[sensor_type]['min'], current_min)\n",
    "                            sensor_stats[sensor_type]['max'] = max(sensor_stats[sensor_type]['max'], current_max)\n",
    "                            sensor_stats[sensor_type]['sum'] += column_data.sum()\n",
    "                            sensor_stats[sensor_type]['count'] += column_data.count()\n",
    "\n",
    "    # Infer units based on sensor type\n",
    "    sensor_units = {\n",
    "        'temperature': '°C',\n",
    "        'humidity': '% RH',\n",
    "        'light': 'Uncalibrated units',\n",
    "        'electric_combined': 'Watts',\n",
    "        'electric_mains': 'Watts',\n",
    "        'electric_subcircuit': 'Watts',\n",
    "        'electric_appliance': 'Watts',\n",
    "        'electric': 'Watts',\n",
    "        'gas_pulse': 'Watt hours',\n",
    "        'gas': 'Watt hours',\n",
    "        'power': 'Watts',\n",
    "        'clamp_temperature': '°C',\n",
    "        # Add more sensor types and their units as needed\n",
    "    }\n",
    "\n",
    "    # Prepare summary data\n",
    "    summary_list = []\n",
    "    for sensor, values in sensor_stats.items():\n",
    "        unit = sensor_units.get(sensor, 'Unknown')\n",
    "        min_val = values['min'] if values['min'] != float('inf') else None\n",
    "        max_val = values['max'] if values['max'] != float('-inf') else None\n",
    "        average_val = values['sum'] / values['count'] if values['count'] > 0 else None\n",
    "        summary_list.append({\n",
    "            'sensor_type': sensor,\n",
    "            'min_value': min_val,\n",
    "            'max_value': max_val,\n",
    "            'average_value': average_val,\n",
    "            'unit': unit\n",
    "        })\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    stats_df = pd.DataFrame(summary_list)\n",
    "    #stats_df.to_csv('sensor_stats.csv', index=False)\n",
    "    #print(\"Sensor stats CSV has been generated as 'sensor_stats.csv'.\")\n",
    "    return stats_df\n",
    "\n",
    "# Call the function to compute min, max, and average values\n",
    "stats_df = compute_sensor_stats(directory, all_sensor_types)\n",
    "print(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Generating Combined Sensor Summary with Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file = 'sensor_summary_combined.csv'\n",
    "\n",
    "# Function to extract general sensor type from the full sensor type string\n",
    "def extract_sensor_type(full_sensor_type):\n",
    "    parts = full_sensor_type.split('_')\n",
    "    return '_'.join(parts[1:])  # Ignore the room part (first element)\n",
    "\n",
    "# Extract sensor types without room prefixes\n",
    "stats_df['clean_sensor_type'] = stats_df['sensor_type'].apply(extract_sensor_type)\n",
    "\n",
    "# Define correct units for each sensor type\n",
    "sensor_units = {\n",
    "    'temperature': '°C',\n",
    "    'humidity': '% RH',\n",
    "    'light': 'Uncalibrated units',\n",
    "    'electric-combined': 'Watts',\n",
    "    'electric': 'Watts',\n",
    "    'electric_appliance': 'Watts',\n",
    "    'gas-pulse_gas': 'Watt hours',\n",
    "    'power': 'Watts',\n",
    "    'clamp_temperature': '°C',\n",
    "    'tempprobe': '°C',\n",
    "    'room_humidity': '% RH',\n",
    "    'room_temperature': '°C',\n",
    "    'room_light': 'Uncalibrated units',\n",
    "    'heater_temperature': '°C',\n",
    "    'heater_humidity': '% RH',\n",
    "    # Add more sensor types and their units as needed\n",
    "}\n",
    "\n",
    "# Function to infer unit from sensor type\n",
    "def infer_unit(sensor_type):\n",
    "    for key in sensor_units:\n",
    "        if key in sensor_type:\n",
    "            return sensor_units[key]\n",
    "    return 'Unknown'\n",
    "\n",
    "# Group by sensor type, ignoring room prefixes, and calculate combined stats\n",
    "summary = stats_df.groupby('clean_sensor_type').agg(\n",
    "    min_value=('min_value', 'min'),\n",
    "    max_value=('max_value', 'max'),\n",
    "    sum_value=('average_value', 'sum'),\n",
    "    count=('average_value', 'count')  # To calculate global average\n",
    ").reset_index()\n",
    "\n",
    "# Calculate average values\n",
    "summary['average_value'] = summary['sum_value'] / summary['count']\n",
    "\n",
    "# Infer units based on sensor type\n",
    "summary['unit'] = summary['clean_sensor_type'].apply(infer_unit)\n",
    "\n",
    "# Drop the sum and count columns as they are no longer needed\n",
    "summary.drop(columns=['sum_value', 'count'], inplace=True)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "summary.rename(columns={'clean_sensor_type': 'sensor_type'}, inplace=True)\n",
    "\n",
    "print(summary)\n",
    "# Save the summary to a new CSV file\n",
    "#summary.to_csv(output_file, index=False)\n",
    "\n",
    "#print(f'Summary has been saved to {output_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Generating Sensor by Room/Device Counts Summary and Summary Statistics for each Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-sensor columns\n",
    "sensor_columns = summary_df.columns.difference(\n",
    "    ['home_number', 'earliest_timestamp', 'latest_timestamp', 'number_of_rows'])\n",
    "\n",
    "# Initialize a nested dictionary to hold counts\n",
    "sensor_room_counts = {}\n",
    "sensor_stats = {}\n",
    "\n",
    "# Process each sensor column to extract room and sensor type\n",
    "for col in sensor_columns:\n",
    "    # Clean the column name by removing numeric suffixes and 'sensor' word\n",
    "    col_cleaned = re.sub(r'\\d+', '', col)\n",
    "    col_cleaned = col_cleaned.replace('sensor', '')\n",
    "    parts = col_cleaned.split('_')\n",
    "\n",
    "    if len(parts) >= 2:\n",
    "        room = parts[0]\n",
    "        sensor_type = '_'.join(parts[1:])\n",
    "    else:\n",
    "        continue  # Skip columns that don't match the expected pattern\n",
    "\n",
    "    # Ensure the sensor_type exists in the dictionary for room counts\n",
    "    if sensor_type not in sensor_room_counts:\n",
    "        sensor_room_counts[sensor_type] = {}\n",
    "    if room not in sensor_room_counts[sensor_type]:\n",
    "        sensor_room_counts[sensor_type][room] = 0\n",
    "\n",
    "    # Count the number of homes where the number of valid datapoints is greater than 0\n",
    "    count_homes = (summary_df[col] > 0).sum()\n",
    "    sensor_room_counts[sensor_type][room] += count_homes\n",
    "\n",
    "    # Calculate min/max statistics across all homes for the sensor\n",
    "    min_value = summary_df[col].min()\n",
    "    max_value = summary_df[col].max()\n",
    "\n",
    "    # Infer units based on sensor type\n",
    "    if 'temperature' in sensor_type:\n",
    "        unit = '°C'\n",
    "    elif 'humidity' in sensor_type:\n",
    "        unit = '%'\n",
    "    elif 'electric' in sensor_type or 'power' in sensor_type:\n",
    "        unit = 'W'\n",
    "    elif 'gas' in sensor_type:\n",
    "        unit = 'Wh'\n",
    "    elif 'light' in sensor_type:\n",
    "        unit = 'Undetermined'\n",
    "    else:\n",
    "        unit = 'Unknown'\n",
    "\n",
    "    # Store sensor statistics\n",
    "    if sensor_type not in sensor_stats:\n",
    "        sensor_stats[sensor_type] = {'min': min_value, 'max': max_value, 'unit': unit}\n",
    "\n",
    "# Convert the nested dictionary for room counts to a DataFrame\n",
    "sensor_summary_df = pd.DataFrame.from_dict(sensor_room_counts, orient='index').fillna(0).astype(int)\n",
    "\n",
    "# Sort the rows and columns for better readability\n",
    "sensor_summary_df = sensor_summary_df.sort_index()\n",
    "sensor_summary_df = sensor_summary_df.reindex(sorted(sensor_summary_df.columns), axis=1)\n",
    "\n",
    "# Save the room summary to CSV\n",
    "#sensor_summary_df.to_csv('room_summary2.csv')\n",
    "print(sensor_summary_df)\n",
    "# Convert sensor stats to DataFrame and save to CSV\n",
    "sensor_stats_df = pd.DataFrame(sensor_stats).T\n",
    "# sensor_stats_df.to_csv('sensor_stats_summary.csv')\n",
    "\n",
    "#print(\"Room summary CSV has been generated as 'room_summary.csv'.\")\n",
    "#print(\"Sensor stats summary CSV has been generated as 'sensor_stats_summary.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Sensor stats summary {sensor_stats_df}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 - Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Cleaning and Processing Files for Electricity and Gas Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "INPUT_DIR = \"./ind-homes\"\n",
    "OUTPUT_DIR = \"./ind-homes-clean\"\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Lists to track ignored files\n",
    "ignored_files = []\n",
    "\n",
    "# Function to clean a single file\n",
    "def clean_file(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Cleans the given CSV file from the ind-homes folder and saves the cleaned version.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the input file.\n",
    "        output_path (str): The path to save the cleaned file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter columns: keep 'timestamp' and columns containing 'elec' or 'gas'\n",
    "    relevant_columns = ['timestamp'] + [col for col in df.columns if 'elec' in col or 'gas' in col]\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # Ensure 'timestamp' is in datetime format\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # Check percentage of missing values for 'elec' and 'gas'\n",
    "    for col in df.columns:\n",
    "        if 'elec' in col or 'gas' in col:\n",
    "            missing_percentage = df[col].isna().mean() * 100\n",
    "            if missing_percentage > 50:\n",
    "                ignored_files.append((os.path.basename(file_path), col, missing_percentage))\n",
    "                return  # Ignore this file completely if one column exceeds 50% missing\n",
    "\n",
    "    # Impute missing values\n",
    "    for col in df.columns:\n",
    "        if 'elec' in col:\n",
    "            df[col] = df[col].ffill().bfill()  # Forward and backward fill\n",
    "        elif 'gas' in col:\n",
    "            df[col] = df[col].fillna(0)  # Fill missing gas values with 0\n",
    "\n",
    "    # Save the cleaned file\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# Function to check for missing values in cleaned files\n",
    "def check_missing_values(directory):\n",
    "    \"\"\"\n",
    "    Checks for missing values in all cleaned files and provides a summary.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing cleaned CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    missing_summary = []\n",
    "    print(\"\\nMissing Value Check:\")\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            missing_counts = df.isna().sum()\n",
    "            total_missing = missing_counts.sum()\n",
    "            \n",
    "            if total_missing > 0:\n",
    "                print(f\"{file_name}: {total_missing} missing values.\")\n",
    "                missing_summary.append((file_name, total_missing))\n",
    "            else:\n",
    "                print(f\"{file_name}: No missing values.\")\n",
    "\n",
    "    if missing_summary:\n",
    "        print(\"\\nSummary of Files with Missing Data:\")\n",
    "        for file, count in missing_summary:\n",
    "            print(f\"{file}: {count} missing values\")\n",
    "    else:\n",
    "        print(\"\\nAll cleaned files have no missing values.\")\n",
    "\n",
    "# Process files in input directory\n",
    "for file_name in os.listdir(INPUT_DIR):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        input_file_path = os.path.join(INPUT_DIR, file_name)\n",
    "        output_file_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "\n",
    "        # Clean and save the file (or ignore if conditions aren't met)\n",
    "        clean_file(input_file_path, output_file_path)\n",
    "        if os.path.exists(output_file_path):\n",
    "            print(f\"Cleaned and saved: {output_file_path}\")\n",
    "\n",
    "# Report ignored files\n",
    "if ignored_files:\n",
    "    print(\"\\nIgnored Files Summary:\")\n",
    "    for file_name, col, percent_missing in ignored_files:\n",
    "        print(f\"{file_name}: Column '{col}' has {percent_missing:.2f}% missing values (> 50%).\")\n",
    "else:\n",
    "    print(\"\\nNo files were ignored due to missing value thresholds.\")\n",
    "\n",
    "# Perform a final check for missing values in cleaned files\n",
    "check_missing_values(OUTPUT_DIR)\n",
    "\n",
    "print(\"All files have been processed, cleaned, and checked for missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Cleaning and Processing Files for Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Inspect Missing Gaps for each Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'raw_weather_data_by_location'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "weather_data = pd.read_csv('raw_data/forecast.csv', delimiter='\\t', parse_dates=['dateforecastfor', 'dateforecastmade'], index_col='dateforecastfor')\n",
    "weather_data = weather_data.sort_index()\n",
    "min_max_temp = weather_data[['locationid', 'maxtemp', 'mintemp']]\n",
    "\n",
    "\n",
    "location_dataframes = {}\n",
    "for loc in min_max_temp['locationid'].unique().tolist():\n",
    "    loc_data = min_max_temp[min_max_temp['locationid'] == loc]\n",
    "    \n",
    "    min_date = loc_data.index.min()\n",
    "    max_date = loc_data.index.max()\n",
    "    \n",
    "    full_date_range = pd.date_range(start=min_date, end=max_date)\n",
    "    \n",
    "    available_dates = loc_data.index\n",
    "    \n",
    "    missing_dates = full_date_range.difference(available_dates)\n",
    "    \n",
    "    print(f\"Location: {loc}\")\n",
    "    print(f\"Min Date: {min_date}\")\n",
    "    print(f\"Max Date: {max_date}\")\n",
    "    print(f\"Total Missing Dates: {len(missing_dates)}\")\n",
    "    print(f\"Missing Date Range: {missing_dates.min()} to {missing_dates.max()}\" if len(missing_dates) > 0 else \"No missing dates\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Create a single df for each location including the missing dates\n",
    "    full_dates_df = pd.DataFrame({'date': full_date_range})\n",
    "    full_dates_df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Merge the complete date range with the location data\n",
    "    merged_data = full_dates_df.merge(loc_data, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Store the data in the location-specific file\n",
    "    file_path = os.path.join(output_dir, f\"{loc}_data.csv\")\n",
    "    merged_data.to_csv(file_path)\n",
    "    \n",
    "    location_dataframes[f\"{loc}_data\"] = merged_data\n",
    "    \n",
    "    print(f\"DataFrame created and saved for location: {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time-series of min and max temp for each location\n",
    "for loc, items in location_dataframes.items():\n",
    "     globals()[f\"{loc.lower()}\"] = items.drop(columns=['locationid'])\n",
    "     \n",
    "for loc, items in location_dataframes.items():\n",
    "     \n",
    "    # Create a figure for max temperature\n",
    "    fig, ax1 = plt.subplots(figsize=(17, 10))\n",
    "    ax1.plot(items.index, items['maxtemp'], linestyle='-', color='blue')\n",
    "    ax1.set_title(f\"Maximum Temperature for {loc}\", fontsize=16)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Max Temperature\", fontsize=14)\n",
    "\n",
    "    # Create a figure for min temperature\n",
    "    fig, ax2 = plt.subplots(figsize=(17, 10))\n",
    "    ax2.plot(items.index, items['mintemp'], linestyle='-', color='orange')\n",
    "    ax2.set_title(f\"Minimum Temperature for {loc}\", fontsize=16)\n",
    "    ax2.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Min Temperature\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Functions to impute missing values for daily timestamps data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for fourier series\n",
    "def fourier_series(x, *a):\n",
    "    \"\"\"General Fourier series function.\"\"\"\n",
    "    ret = a[0] / 2  # The constant term\n",
    "    n_terms = (len(a) - 1) // 2\n",
    "    for i in range(n_terms):\n",
    "        ret += a[1 + i] * np.sin((i + 1) * x * 2 * np.pi / 365) + a[1 + n_terms + i] * np.cos((i + 1) * x * 2 * np.pi / 365)\n",
    "    return ret\n",
    "\n",
    "# function that uses the fourier series predictions on the observed data to calculate the residuals with the observed data and choose a random residual to add it in the predicted values to account for ups and downs\n",
    "def add_residual_variation(predicted, residuals):\n",
    "    \"\"\"Add random residual noise to predicted values.\"\"\"\n",
    "    if len(residuals) > 0:\n",
    "        noise = np.random.choice(residuals, size=len(predicted), replace=True)\n",
    "        return predicted + noise\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def impute_gaps(location_data, column_to_impute, small_gap_threshold=6, debug=False):\n",
    "    # Ensure the index is datetime\n",
    "    if not isinstance(location_data.index, pd.DatetimeIndex):\n",
    "        location_data.index = pd.to_datetime(location_data.index)\n",
    "\n",
    "    # Mark missing values for the specified column\n",
    "    is_missing_col = f'is_missing_{column_to_impute}'\n",
    "    gap_group_col = f'gap_group_{column_to_impute}'\n",
    "\n",
    "    location_data[is_missing_col] = location_data[column_to_impute].isna()\n",
    "\n",
    "    # Identify groups of consecutive missing values\n",
    "    location_data[gap_group_col] = (\n",
    "        location_data[is_missing_col] != location_data[is_missing_col].shift()  # keeps track of the previous row's record (shift) and allows the comparison with the current row to identify if the rows are in the same group (either True or False)\n",
    "    ).cumsum() #  here cumsum accounts for the comparison --> if the comparison is True is means that we have a shift to a new group (cumsum increases) while if the comparison is false it means we are in the same group so the cumsum remains the same --> this way we group the rows based on the comparisons we did\n",
    "\n",
    "    # Calculate the size of each gap group\n",
    "    gap_sizes = location_data.groupby(gap_group_col).agg(\n",
    "        is_missing=(is_missing_col, 'first'), # stores True or False in the is_missing column to determine if a missing gap starts or no\n",
    "        gap_size=(is_missing_col, 'sum') \n",
    "    ).reset_index()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Gap Sizes for {column_to_impute}:\")\n",
    "        print(gap_sizes)\n",
    "\n",
    "    # Handle small gaps (≤ small_gap_threshold)\n",
    "    small_gaps = gap_sizes[gap_sizes['is_missing'] & (gap_sizes['gap_size'] <= small_gap_threshold)]\n",
    "    if debug:\n",
    "        print(f\"Identified small gaps (≤ {small_gap_threshold}): {len(small_gaps)}\")\n",
    "\n",
    "    for _, row in small_gaps.iterrows():\n",
    "        gap_group = row[gap_group_col]\n",
    "        gap_indices = location_data[location_data[gap_group_col] == gap_group].index\n",
    "        if debug:\n",
    "            print(f\"Small gap group {gap_group} from {gap_indices[0]} to {gap_indices[-1]}\")\n",
    "\n",
    "        # Apply spline interpolation for small gaps\n",
    "        location_data.loc[gap_indices, column_to_impute] = location_data[column_to_impute].interpolate(\n",
    "            method='spline', order=3\n",
    "        )\n",
    "\n",
    "    # Handle large gaps (> small_gap_threshold)\n",
    "    large_gaps = gap_sizes[gap_sizes['is_missing'] & (gap_sizes['gap_size'] > small_gap_threshold)]\n",
    "    if debug:\n",
    "        print(f\"Identified large gaps (> {small_gap_threshold}): {len(large_gaps)}\")\n",
    "\n",
    "    for _, row in large_gaps.iterrows():\n",
    "        gap_group = row[gap_group_col]\n",
    "        gap_indices = location_data[location_data[gap_group_col] == gap_group].index\n",
    "        gap_start = gap_indices[0]\n",
    "        gap_end = gap_indices[-1]\n",
    "\n",
    "        if debug:\n",
    "            print(f\"gap_start: {gap_start}, gap_end: {gap_end}\")\n",
    "            print(f\"Data index type: {location_data.index}\")\n",
    "\n",
    "        try:\n",
    "            # Extract available data (excluding missing values)\n",
    "            available_data = location_data[~location_data[column_to_impute].isna()]\n",
    "            x = (available_data.index - available_data.index[0]).days.values # transform dates in numerical format (by day)\n",
    "            y = available_data[column_to_impute].values # extract the values of the existing data for the selected column\n",
    "\n",
    "            # Fit a Fourier series\n",
    "            n_harmonics = 3  # Number of harmonics to capture seasonal variation (determines how many sine and cosine functions are used to capture the seasonal variation)\n",
    "            p0 = [np.mean(y)] + [0] * (2 * n_harmonics)  # Initial guess (necessary for the curve_fit function to start the optimisation)\n",
    "            params, _ = curve_fit(fourier_series, x, y, p0=p0) # use the curve_fit to extract the optimal params for the Fourier Series that minimise the difference between the observed and the fourier predicted values (minimises the sum of square residuals)\n",
    "            \n",
    "            # Explanation of how we define harmonics (based on the visual representation of the data:\n",
    "            # The first harmonic captures the broad seasonal cycle (e.g., summer to winter transitions).\n",
    "            # The second harmonic refines this by accounting for mid-season shifts.\n",
    "            # The third harmonic focuses on even smaller variations, like monthly changes or irregularities within seasons.\n",
    "            # the more harmonics we add the more the detail but then the model is prone to overfitting\n",
    "            \n",
    "            #COSINE: It starts at a peak (1), dips to zero, drops to a trough (−1), returns to zero, and completes a cycle back at 1 ! better for long-term oscillations\n",
    "            #SINE: It starts at zero, rises to a peak (1), falls to zero, dips to a trough (−1), and completes a cycle back at 0 ! better for mid-season oscillations since it starts from zero\n",
    "            # Since there two together are orthogonal, they capture different variations\n",
    "            \n",
    "\n",
    "            \n",
    "            # Calculate residuals for the \n",
    "            y_fit = fourier_series(x, *params) # params = a0, ak, bk. These represent the average level of the data (a0), the amplitude of the  k-term cosine (ak) and the amplitude of the k-th sine term (bk)\n",
    "            residuals = y - y_fit\n",
    "\n",
    "            # Predict for the gap indices\n",
    "            x_pred = (gap_indices - available_data.index[0]).days.values # extract the dates in numerical format for the missing timestamps\n",
    "            y_pred = fourier_series(x_pred, *params) # apply the fourier series to predict the missing gaps using the optimised params that were obtained from the fitted fourier model on the existing data\n",
    "\n",
    "            # Add residual-based noise \n",
    "            y_pred_with_noise = add_residual_variation(y_pred, residuals) # we add to the predicted values of the missing gaps, a random noise that was obtained based on the residuals that were calculated between the predicted (from fourier series) and the observed data\n",
    "\n",
    "            # Assign predicted values\n",
    "            location_data.loc[gap_indices, column_to_impute] = y_pred_with_noise\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Fourier series imputed values with noise for gap group {gap_group}:\")\n",
    "                print(y_pred_with_noise)\n",
    "\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"Fourier series failed for gap group {gap_group}: {e}\")\n",
    "            # Fallback to KNN\n",
    "            print(f\"Fallback to KNN for gap group {gap_group}.\")\n",
    "            knn_imputer = KNNImputer(n_neighbors=3)\n",
    "            location_data[column_to_impute] = knn_imputer.fit_transform(\n",
    "                location_data[[column_to_impute]]\n",
    "            ).flatten()\n",
    "\n",
    "    # Drop temporary columns specific to the current imputation\n",
    "    location_data.drop(columns=[is_missing_col, gap_group_col], inplace=True)\n",
    "\n",
    "    # Final debugging\n",
    "    if debug:\n",
    "        print(f\"Final missing values after imputation for {column_to_impute}: {location_data[column_to_impute].isna().sum()}\")\n",
    "\n",
    "    return location_data\n",
    "\n",
    "\n",
    "\n",
    "# Apply imputation for each location\n",
    "for loc, data in location_dataframes.items():\n",
    "    print(f\"Processing location: {loc}\")\n",
    "    \n",
    "    # Impute 'mintemp'\n",
    "    location_dataframes[loc] = impute_gaps(data, column_to_impute='mintemp', debug=True)\n",
    "    print(f\"Processed missing data for mintemp for location: {loc}\")\n",
    "\n",
    "    # Impute 'maxtemp'\n",
    "    location_dataframes[loc] = impute_gaps(location_dataframes[loc], column_to_impute='maxtemp', debug=True)\n",
    "    print(f\"Processed missing data for maxtemp for location: {loc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the imputed min and max temp by location\n",
    "for loc, items in location_dataframes.items():\n",
    "     \n",
    "    # Create a figure for max temperature\n",
    "    fig, ax1 = plt.subplots(figsize=(17, 10))\n",
    "    ax1.plot(items.index, items['maxtemp'], linestyle='-', color='blue')\n",
    "    ax1.set_title(f\"Maximum Temperature for {loc}\", fontsize=16)\n",
    "    ax1.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Max Temperature\", fontsize=14)\n",
    "\n",
    "    # Create a figure for min temperature\n",
    "    fig, ax2 = plt.subplots(figsize=(17, 10))\n",
    "    ax2.plot(items.index, items['mintemp'], linestyle='-', color='orange')\n",
    "    ax2.set_title(f\"Minimum Temperature for {loc}\", fontsize=16)\n",
    "    ax2.set_xlabel(\"Date\", fontsize=14)\n",
    "    ax2.set_ylabel(\"Min Temperature\", fontsize=14)\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Build hourly observations for min and max temp for each location (separately)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 Calculate hourly timestamps for each location based on their min and max timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_timestamps_for_each_location(location):\n",
    "    hourly_timestamps = pd.date_range(start=location_dataframes[location].index.strftime('%Y-%m-%d').min(),\n",
    "                                    end =location_dataframes[location].index.strftime('%Y-%m-%d').max(),\n",
    "                                    freq='H')\n",
    "\n",
    "    hourly_timestamps = pd.to_datetime(hourly_timestamps)\n",
    "    hourly_timestamps = pd.DataFrame(hourly_timestamps, columns=['timestamp']) \n",
    "    \n",
    "    return hourly_timestamps\n",
    "\n",
    "hourly_timestamps_by_location = {}\n",
    "\n",
    "for location in location_dataframes.keys():\n",
    "    hourly_timestamps_by_location[f'{location}_hourly_timestamps'] = hourly_timestamps_for_each_location(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.2 Estimate the hourly min and max temp for each location (sine function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_temps_estimation(mintemp, maxtemp):\n",
    "    avg_temp = np.mean([mintemp, maxtemp])\n",
    "    amplitude = (np.ptp([mintemp, maxtemp]))/2\n",
    "    \n",
    "    t_min = 5\n",
    "    hourly_temps = []\n",
    "    \n",
    "    for hour in range(24):\n",
    "        temp = avg_temp + amplitude * (np.sin((2 * np.pi * (hour-t_min))/24 - (5 * np.pi / 12)))\n",
    "        \n",
    "        hourly_temps.append(temp)\n",
    "    \n",
    "    return hourly_temps\n",
    "\n",
    "\n",
    "hourly_temps_by_location_dict = {}\n",
    "\n",
    "for location in location_dataframes.keys():\n",
    "\n",
    "    hourly_temps_by_location_dict[location] = {}\n",
    "    \n",
    "    for date, row in location_dataframes[location].iterrows():\n",
    "        # Store hourly temperature estimates for each date\n",
    "        hourly_temps_by_location_dict[location][date] = hourly_temps_estimation(row['mintemp'], row['maxtemp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'cleaned_weather_data_by_location'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define function to retrieve temperature based on timestamp\n",
    "def get_temperature(ts, hourly_temps_dict, location):\n",
    "    # converts the timestamps of hourly_timestamp (hours,minutes and seconds) into midnight time to match the key of hourly_temps_dict and stores it to a variable\n",
    "    date_key = ts.normalize()\n",
    "    \n",
    "    # for each normalised date_key, it returns the temperature that corresponds to the 'hour' component of the hourly_timestamps \n",
    "    # which means that if the ts.hour is 15 then it will extract the 15th element of the dictionary to match it.\n",
    "    if location in hourly_temps_dict:\n",
    "        \n",
    "        if date_key in hourly_temps_dict[location]:\n",
    "\n",
    "            # Fetch temperature for the hour\n",
    "            return hourly_temps_dict[location][date_key][ts.hour]\n",
    "        else:\n",
    "            print(f\"Date '{date_key}' NOT found for location '{location}'.\")\n",
    "    else:\n",
    "        print(f\"Location '{location}' NOT found in hourly_temps_dict.\")\n",
    "\n",
    "    # Return None if no match is found\n",
    "    return None\n",
    "\n",
    "# Iterate through locations and assign temperature\n",
    "for location_with_suffix in hourly_timestamps_by_location.keys():\n",
    "    # Extract the base location name\n",
    "    location = location_with_suffix.replace('_hourly_timestamps', '')\n",
    "\n",
    "    # Get the DataFrame for the current location\n",
    "    location_df = hourly_timestamps_by_location[location_with_suffix]\n",
    "\n",
    "    # Apply the temperature assignment\n",
    "    location_df['temperature'] = location_df['timestamp'].apply(\n",
    "        get_temperature, args=(hourly_temps_by_location_dict, location)\n",
    "    )\n",
    "\n",
    "    # Update the dictionary with the modified DataFrame\n",
    "    hourly_timestamps_by_location[location_with_suffix] = location_df\n",
    "   \n",
    "   \n",
    "    \n",
    "# Plot hourly temperatures for each location\n",
    "for location, temps in hourly_timestamps_by_location.items():\n",
    "    hourly_timestamps_by_location[location]['timestamp'] = pd.to_datetime(hourly_timestamps_by_location[location]['timestamp'])\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(hourly_timestamps_by_location[location]['timestamp'], hourly_timestamps_by_location[location]['temperature'], color='blue', label='Hourly Temperature')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.title(f'Hourly Temperature Variation Over Time for {location.split(\"_\")[0]}')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45) \n",
    "    plt.tight_layout()  \n",
    "    plt.show()\n",
    "    \n",
    "# Store the files in the specified directory\n",
    "for location_with_suffix, df in hourly_timestamps_by_location.items():\n",
    "    base_location = location_with_suffix.replace('_hourly_timestamps', '')\n",
    "    # Construct the full file path to save in the output directory\n",
    "    file_path = os.path.join(output_dir, f\"hourly_temperatures_{base_location}.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Data for {base_location} saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Merging Metadata to Gas and Electricity Data for each House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "METADATA_FILE = r'./raw_data/home_metadata.csv'\n",
    "DATA_DIR = 'ind-homes-clean'\n",
    "OUTPUT_DIR = 'ind-homes-clean-modified'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load the home metadata into a DataFrame\n",
    "metadata_df = pd.read_csv(METADATA_FILE)\n",
    "\n",
    "# Iterate over each CSV file in the data directory\n",
    "for filename in os.listdir(DATA_DIR):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        \n",
    "        # Extract the home ID from the filename\n",
    "        # e.g., 'home_home306.csv' -> '306'\n",
    "        parts = filename.replace('.csv', '').split('_')\n",
    "        if len(parts) >= 2:\n",
    "            home_id_str = parts[1].replace('home', '')\n",
    "        else:\n",
    "            continue  # Skip files that don't match the expected pattern\n",
    "\n",
    "        # Convert home ID to integer\n",
    "        try:\n",
    "            home_id = int(home_id_str)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid home ID in filename: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Find the metadata for this home ID\n",
    "        home_metadata = metadata_df[metadata_df['homeid'] == home_id]\n",
    "        if home_metadata.empty:\n",
    "            print(f\"No metadata found for home ID {home_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Get the location and other metadata\n",
    "        location = home_metadata['location'].values[0]\n",
    "        metadata_dict = home_metadata.to_dict(orient='records')[0]\n",
    "        metadata_dict.pop('homeid', None)  # Remove 'homeid' from metadata\n",
    "\n",
    "        # Load the home's data file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Rename the second column to 'elec' and third column to 'gas'\n",
    "        if df.shape[1] >= 3:\n",
    "            df.columns.values[1] = 'elec'\n",
    "            df.columns.values[2] = 'gas'\n",
    "\n",
    "        # Add metadata columns to the DataFrame\n",
    "        for key, value in metadata_dict.items():\n",
    "            df[key] = value  # Fill the entire column with the metadata value\n",
    "\n",
    "        # Construct the new filename\n",
    "        new_filename = f\"{home_id}_{location}.csv\"\n",
    "        new_file_path = os.path.join(OUTPUT_DIR, new_filename)\n",
    "\n",
    "        # Save the modified DataFrame\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "\n",
    "        print(f\"Processed {filename} -> {new_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Merging Weather Data to Gas and Electricity Data for each House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "INPUT_DIR = 'ind-homes-clean-modified'\n",
    "WEATHER_DATA_DIR = 'cleaned_weather_data_by_location'\n",
    "OUTPUT_DIR = 'ind-homes-with-weather'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Iterate over each CSV file in the modified data directory\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if filename.endswith('.csv'):\n",
    "        try:\n",
    "            file_path = os.path.join(INPUT_DIR, filename)\n",
    "            \n",
    "            # Extract the location from the filename\n",
    "            # e.g., '306_Edinburgh.csv' -> 'Edinburgh'\n",
    "            parts = filename.replace('.csv', '').split('_')\n",
    "            if len(parts) >= 2:\n",
    "                location = parts[1]\n",
    "            else:\n",
    "                print(f\"Could not extract location from filename: {filename}\")\n",
    "                continue  # Skip files that don't match the expected pattern\n",
    "            \n",
    "            # Construct the weather data filename\n",
    "            weather_filename = f'hourly_temperatures_{location}_data.csv'\n",
    "            weather_file_path = os.path.join(WEATHER_DATA_DIR, weather_filename)\n",
    "            \n",
    "            # Check if the weather data file exists\n",
    "            if not os.path.exists(weather_file_path):\n",
    "                print(f\"Weather data file not found for location '{location}': {weather_filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Load the home's data file\n",
    "            home_df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Load the weather data file\n",
    "            weather_df = pd.read_csv(weather_file_path)\n",
    "            \n",
    "            # Ensure 'timestamp' columns are in datetime format\n",
    "            home_df['timestamp'] = pd.to_datetime(home_df['timestamp'], errors='coerce')\n",
    "            weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp'], errors='coerce')\n",
    "            \n",
    "            # Merge the weather data into the home data on 'timestamp'\n",
    "            merged_df = pd.merge(home_df, weather_df[['timestamp', 'temperature']], on='timestamp', how='left')\n",
    "            \n",
    "            # Insert the temperature column after 'gas'\n",
    "            cols = list(merged_df.columns)\n",
    "            gas_index = cols.index('gas')\n",
    "            # Remove 'temperature' column and re-insert it after 'gas'\n",
    "            cols.remove('temperature')\n",
    "            cols.insert(gas_index + 1, 'temperature')\n",
    "            merged_df = merged_df[cols]\n",
    "            \n",
    "            # Construct the output file path\n",
    "            output_file_path = os.path.join(OUTPUT_DIR, filename)\n",
    "            \n",
    "            # Save the updated DataFrame to the output directory\n",
    "            merged_df.to_csv(output_file_path, index=False)\n",
    "            \n",
    "            print(f\"Processed and saved {filename} to {output_file_path}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {filename} due to an error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'ind-homes-with-weather'\n",
    "dest_dir = \"ind-homes-final\"\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# Columns to drop\n",
    "columns_to_remove = [\n",
    "    \"starttime\", \n",
    "    \"starttime_enhanced\", \n",
    "    \"endtime\", \n",
    "    \"cohortid\", \n",
    "    \"urban_rural_class\", \n",
    "    \"new_build_year\"\n",
    "]\n",
    "\n",
    "# Iterate through each CSV file in the source directory\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".csv\"):  # Process only CSV files\n",
    "        # Define full paths for source and destination files\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        dest_file = os.path.join(dest_dir, filename)\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(source_file)\n",
    "\n",
    "        # Drop the specified columns\n",
    "        df_cleaned = df.drop(columns=columns_to_remove, errors='ignore')\n",
    "\n",
    "        # Save the cleaned data to the destination directory\n",
    "        df_cleaned.to_csv(dest_file, index=False)\n",
    "\n",
    "        print(f\"Processed: {filename}\")\n",
    "\n",
    "print(f\"Cleaned CSV files have been saved to: {dest_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 - Model Preprocessing and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"ind-homes-final\"\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Preprocesses input files: handle missing data, ensure timestamps,\n",
    "    encode categorical variables, and select key fields.\n",
    "    \n",
    "    Returns the DataFrame with dummy-encoded columns (if any object dtype).\n",
    "    \"\"\"\n",
    "    # Ensure 'timestamp' is in datetime format and set as index\n",
    "    if \"timestamp\" in df.columns:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "        df.set_index(\"timestamp\", inplace=True)\n",
    "    \n",
    "    # Reindex to ensure consistent hourly frequency\n",
    "    if len(df.index) > 0:\n",
    "        df = df.reindex(pd.date_range(df.index.min(), df.index.max(), freq=\"h\"))\n",
    "    \n",
    "    # Fill missing values for 'elec' and 'gas'\n",
    "    if \"elec\" in df.columns:\n",
    "        df[\"elec\"] = df[\"elec\"].interpolate(method=\"linear\").bfill().ffill()\n",
    "    if \"gas\" in df.columns:\n",
    "        df[\"gas\"] = df[\"gas\"].fillna(0)\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Encode categorical variables using one-hot encoding\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Fill any remaining missing values with zeros\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Compute RMSE for predictions.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return rmse if np.isfinite(rmse) else None\n",
    "\n",
    "def random_household_selection(num_files=1, exclude=[]):\n",
    "    \"\"\"Randomly selects `num_files` household files, excluding the specified ones.\"\"\"\n",
    "    all_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".csv\")]\n",
    "    available_files = [f for f in all_files if f not in exclude]\n",
    "    return random.sample(available_files, num_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 - Model Training and Comparison\n",
    "1. Arima (gas/electricity)\n",
    "2. Sarimax (gas/electricity + weather + metadata)\n",
    "3. LSTM (gas/electricity + weather + metadata for one home)\n",
    "4. LSTM (gas/electricity + weather + metadata for all homes)\n",
    "5. Prophet (gas/electricity for one home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Functions for Time Series Analysis and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 Arima Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR ARIMA\n",
    "def check_stationarity(series, series_name):\n",
    "    \"\"\"Perform the Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
    "    result = adfuller(series)\n",
    "    print(f\"ADF Statistic for {series_name}: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"The {series_name} series is stationary.\")\n",
    "    else:\n",
    "        print(f\"The {series_name} series is not stationary.\")\n",
    "\n",
    "def fit_arima_and_plot(df, col, home_name):\n",
    "    \"\"\"\n",
    "    Fit ARIMA model and plot fitted vs actual and residual plots for the specified column.\n",
    "    \"\"\"\n",
    "    \"\"\"Train ARIMA model using fixed order (1, 0, 1) for the target variable.\"\"\"\n",
    "    # Stationarity check on electricity consumption\n",
    "    print(\"Performing stationarity check on electricity...\")\n",
    "    check_stationarity(df[\"elec\"], \"Electricity Consumption\")\n",
    "    \n",
    "    order = (1, 0, 1)\n",
    "    model = ARIMA(df[col], order=order)\n",
    "    fitted_model = model.fit()\n",
    "    df[\"fitted\"] = fitted_model.fittedvalues\n",
    "   \n",
    "    \n",
    "    # Fitted vs Actual chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df[col], label=\"Actual\", color=\"blue\")\n",
    "    plt.plot(df.index, df[\"fitted\"], label=\"Fitted\", color=\"orange\")\n",
    "    plt.title(f\"{home_name} - {col.capitalize()} - Fitted vs Actual\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals chart\n",
    "    residuals = df[col] - df[\"fitted\"]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(residuals, label=\"Residuals\", color=\"purple\")\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    plt.title(f\"{home_name} - {col.capitalize()} - Residuals\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    return fitted_model\n",
    "\n",
    "\n",
    "def evaluate_arima():\n",
    "    \"\"\"Train and evaluate ARIMA on a single random household, test on 5 random households.\"\"\"\n",
    "    train_file = random_household_selection(1)[0]\n",
    "    df_train = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, train_file)))\n",
    "    train_cols = df_train.columns\n",
    "\n",
    "    test_files = random_household_selection(5, exclude=[train_file])\n",
    "    \n",
    "    home_name = train_file.split(\"_\")[0]\n",
    "    rmse_dict = {\"elec\": [], \"gas\": []}\n",
    "\n",
    "    for target in [\"elec\", \"gas\"]:\n",
    "        if target not in df_train.columns:\n",
    "            continue\n",
    "\n",
    "        arima_model = fit_arima_and_plot(df_train, target, home_name)\n",
    "        for test_file in test_files:\n",
    "            df_test = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, test_file)))\n",
    "            df_test = df_test.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "            if target not in df_test.columns:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                forecast = arima_model.forecast(steps=len(df_test))\n",
    "                rmse = evaluate_model(df_test[target], forecast)\n",
    "                if rmse is not None:\n",
    "                    rmse_dict[target].append(rmse)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    for t in rmse_dict:\n",
    "        rmse_dict[t] = np.mean(rmse_dict[t]) if rmse_dict[t] else None\n",
    "    return rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Sarimax Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR SARIMAX\n",
    "def fit_sarimax(df, target_col):\n",
    "    \"\"\"Train SARIMAX model using extra covariates (all other columns).\"\"\"\n",
    "    exogenous_columns = df.columns.difference([target_col])\n",
    "    exog = df[exogenous_columns]\n",
    "\n",
    "    # Ensure exogenous variables are numeric and no missing\n",
    "    exog = exog.select_dtypes(include=[np.number])\n",
    "    exog.fillna(0, inplace=True)\n",
    "\n",
    "    model = SARIMAX(df[target_col], exog=exog, order=(1, 0, 1))\n",
    "    fitted_model = model.fit(disp=False)\n",
    "    return fitted_model\n",
    "\n",
    "def evaluate_sarimax():\n",
    "    \"\"\"Train and evaluate SARIMAX on a single random household, test on 5 random households.\"\"\"\n",
    "    train_file = random_household_selection(1)[0]\n",
    "    df_train = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, train_file)))\n",
    "    train_cols = df_train.columns\n",
    "\n",
    "    test_files = random_household_selection(5, exclude=[train_file])\n",
    "    rmse_dict = {\"elec\": [], \"gas\": []}\n",
    "\n",
    "    for target in [\"elec\", \"gas\"]:\n",
    "        if target not in df_train.columns:\n",
    "            continue\n",
    "\n",
    "        sarimax_model = fit_sarimax(df_train, target_col=target)\n",
    "        for test_file in test_files:\n",
    "            df_test = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, test_file)))\n",
    "            df_test = df_test.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "            if target not in df_test.columns:\n",
    "                continue\n",
    "\n",
    "            exog_test = df_test[df_test.columns.difference([target])]\n",
    "            exog_test = exog_test.select_dtypes(include=[np.number]).fillna(0)\n",
    "\n",
    "            try:\n",
    "                forecast = sarimax_model.forecast(steps=len(df_test), exog=exog_test)\n",
    "                forecast = forecast[:len(df_test[target])]\n",
    "                rmse = evaluate_model(df_test[target], forecast)\n",
    "                if rmse is not None:\n",
    "                    rmse_dict[target].append(rmse)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during SARIMAX evaluation for {test_file}: {e}\")\n",
    "\n",
    "    for t in rmse_dict:\n",
    "        rmse_dict[t] = np.mean(rmse_dict[t]) if rmse_dict[t] else None\n",
    "    return rmse_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 LSTM On Single Household Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR LSTM ON A SINGLE HOUSEHOLD\n",
    "def fit_lstm_single(df, target_col):\n",
    "    \"\"\"Train LSTM model on one household for a single target variable.\"\"\"\n",
    "    # Drop rows where target is missing\n",
    "    df = df.dropna(subset=[target_col])\n",
    "    \n",
    "    # Separate features/target\n",
    "    data = df.drop(columns=[target_col]).values\n",
    "    target = df[target_col].values\n",
    "\n",
    "    # Scale features and target\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    data_scaled = scaler_X.fit_transform(data)\n",
    "    target_scaled = scaler_y.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "    # Create sequences\n",
    "    sequence_len = 24  # Use previous 24 hours as input\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_len, len(data_scaled)):\n",
    "        X.append(data_scaled[i - sequence_len:i])\n",
    "        y.append(target_scaled[i])\n",
    "    X, y = np.array(X), np.array(y)\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        Input(shape=(X.shape[1], X.shape[2])),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    return model, scaler_X, scaler_y\n",
    "\n",
    "def evaluate_lstm_single():\n",
    "    \"\"\"Train and evaluate LSTM on a single random household, test on 5 random households.\"\"\"\n",
    "    train_file = random_household_selection(1)[0]\n",
    "    df_train = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, train_file)))\n",
    "    train_cols = df_train.columns\n",
    "\n",
    "    test_files = random_household_selection(5, exclude=[train_file])\n",
    "    rmse_dict = {\"elec\": [], \"gas\": []}\n",
    "\n",
    "    for target in [\"elec\", \"gas\"]:\n",
    "        if target not in df_train.columns:\n",
    "            continue\n",
    "\n",
    "        lstm_model, scaler_X, scaler_y = fit_lstm_single(df_train, target)\n",
    "\n",
    "        for test_file in test_files:\n",
    "            df_test = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, test_file)))\n",
    "            df_test = df_test.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "            if target not in df_test.columns:\n",
    "                continue\n",
    "            df_test = df_test.dropna(subset=[target])\n",
    "\n",
    "            data_test = df_test.drop(columns=[target]).values\n",
    "            target_test = df_test[target].values\n",
    "\n",
    "            data_test_scaled = scaler_X.transform(data_test)\n",
    "\n",
    "            sequence_len = 24\n",
    "            X_test = []\n",
    "            for i in range(sequence_len, len(data_test_scaled)):\n",
    "                X_test.append(data_test_scaled[i - sequence_len:i])\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = target_test[sequence_len:]\n",
    "\n",
    "            y_pred_scaled = lstm_model.predict(X_test)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "            rmse = evaluate_model(y_test, y_pred.flatten())\n",
    "            if rmse is not None:\n",
    "                rmse_dict[target].append(rmse)\n",
    "\n",
    "    for t in rmse_dict:\n",
    "        rmse_dict[t] = np.mean(rmse_dict[t]) if rmse_dict[t] else None\n",
    "    return rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4 LSTM On Multiple Households Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR LSTM SEQUENTIAL TRAINING ON MULTIPLE HOUSEHOLDS\n",
    "def fit_lstm_sequential(train_dfs, target_col):\n",
    "    \"\"\"\n",
    "    Train LSTM model sequentially on multiple DataFrames (already preprocessed & reindexed).\n",
    "    Returns the final model and the scalers.\n",
    "    \"\"\"\n",
    "    model, scaler_X, scaler_y = None, None, None\n",
    "    sequence_len = 24\n",
    "\n",
    "    for idx, df in enumerate(train_dfs):\n",
    "        df = df.dropna(subset=[target_col])\n",
    "        data = df.drop(columns=[target_col]).values\n",
    "        target = df[target_col].values\n",
    "\n",
    "        # Initialize scalers if first iteration\n",
    "        if model is None:\n",
    "            # First file dictates the shape and initializes everything\n",
    "            scaler_X = MinMaxScaler()\n",
    "            scaler_y = MinMaxScaler()\n",
    "\n",
    "            data_scaled = scaler_X.fit_transform(data)\n",
    "            target_scaled = scaler_y.fit_transform(target.reshape(-1, 1))\n",
    "\n",
    "            # Build model\n",
    "            model = Sequential([\n",
    "                Input(shape=(sequence_len, data_scaled.shape[1])),\n",
    "                LSTM(50, return_sequences=True),\n",
    "                LSTM(50),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        else:\n",
    "            # For subsequent files, transform with existing scalers\n",
    "            data_scaled = scaler_X.transform(data)\n",
    "            target_scaled = scaler_y.transform(target.reshape(-1, 1))\n",
    "\n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(sequence_len, len(data_scaled)):\n",
    "            X.append(data_scaled[i - sequence_len:i])\n",
    "            y.append(target_scaled[i])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "\n",
    "        # Train the model incrementally\n",
    "        model.fit(X, y, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    return model, scaler_X, scaler_y\n",
    "\n",
    "def evaluate_lstm_sequential():\n",
    "    \"\"\"\n",
    "    Train LSTM sequentially on 20 random households, then evaluate on 5 random households,\n",
    "    returning separate RMSE for elec and gas.\n",
    "    \"\"\"\n",
    "    train_files = random_household_selection(20)\n",
    "    test_files = random_household_selection(5, exclude=train_files)\n",
    "\n",
    "    train_dfs = []\n",
    "    union_train_cols = set()\n",
    "\n",
    "    for f in train_files:\n",
    "        df_temp = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, f)))\n",
    "        train_dfs.append(df_temp)\n",
    "        union_train_cols = union_train_cols.union(df_temp.columns)\n",
    "    union_train_cols = list(union_train_cols)\n",
    "\n",
    "    # Reindex each train df to the union of columns\n",
    "    for idx, df_temp in enumerate(train_dfs):\n",
    "        train_dfs[idx] = df_temp.reindex(columns=union_train_cols, fill_value=0)\n",
    "\n",
    "    rmse_dict = {\"elec\": [], \"gas\": []}\n",
    "\n",
    "    for target in [\"elec\", \"gas\"]:\n",
    "        if target not in union_train_cols:\n",
    "            continue\n",
    "\n",
    "        lstm_model, scaler_X, scaler_y = fit_lstm_sequential(train_dfs, target)\n",
    "\n",
    "        # Evaluate on test files\n",
    "        for test_file in test_files:\n",
    "            df_test = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, test_file)))\n",
    "            df_test = df_test.reindex(columns=union_train_cols, fill_value=0)\n",
    "\n",
    "            if target not in df_test.columns:\n",
    "                continue\n",
    "            df_test = df_test.dropna(subset=[target])\n",
    "\n",
    "            data_test = df_test.drop(columns=[target]).values\n",
    "            target_test = df_test[target].values\n",
    "            data_test_scaled = scaler_X.transform(data_test)\n",
    "\n",
    "            sequence_len = 24\n",
    "            X_test = []\n",
    "            for i in range(sequence_len, len(data_test_scaled)):\n",
    "                X_test.append(data_test_scaled[i - sequence_len:i])\n",
    "            X_test = np.array(X_test)\n",
    "            y_true = target_test[sequence_len:]\n",
    "\n",
    "            y_pred_scaled = lstm_model.predict(X_test)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "            rmse = evaluate_model(y_true, y_pred.flatten())\n",
    "            if rmse is not None:\n",
    "                rmse_dict[target].append(rmse)\n",
    "\n",
    "    for t in rmse_dict:\n",
    "        rmse_dict[t] = np.mean(rmse_dict[t]) if rmse_dict[t] else None\n",
    "    return rmse_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5 Phophet on Single Household Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Prophet training on a single household and evaluation of the model using other households' data\n",
    "def fit_prophet(df, target_col):\n",
    "    \"\"\"\n",
    "    Train a Prophet model using the given DataFrame (with 'timestamp' as index) for target_col.\n",
    "    We temporarily reset the index, rename columns to Prophet's required format (ds, y),\n",
    "    fit, and return the model.\n",
    "    \"\"\"\n",
    "    # Prophet requires a column named 'ds' for time and 'y' for the target\n",
    "    # Temporarily reset index to a column\n",
    "    df_prophet = df.reset_index().rename(columns={'index': 'ds', target_col: 'y'})\n",
    "    \n",
    "    # Only keep ds and y\n",
    "    df_prophet = df_prophet[['ds', 'y']]\n",
    "    \n",
    "    # Prophet requires no missing values\n",
    "    df_prophet.dropna(inplace=True)\n",
    "    \n",
    "    # Instantiate and fit Prophet\n",
    "    model = Prophet()\n",
    "    model.fit(df_prophet)\n",
    "    return model\n",
    "\n",
    "def evaluate_prophet():\n",
    "    \"\"\"\n",
    "    Train and evaluate a Prophet model on a single random household (train),\n",
    "    then test on 5 random households. Return average RMSE for 'elec' and 'gas'.\n",
    "    \"\"\"\n",
    "    if Prophet is None:\n",
    "        print(\"Prophet not installed. Skipping Prophet evaluation.\")\n",
    "        return {\"elec\": None, \"gas\": None}\n",
    "\n",
    "    train_file = random_household_selection(1)[0]\n",
    "    df_train = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, train_file)))\n",
    "    train_cols = df_train.columns\n",
    "\n",
    "    test_files = random_household_selection(5, exclude=[train_file])\n",
    "    rmse_dict = {\"elec\": [], \"gas\": []}\n",
    "\n",
    "    for target in [\"elec\", \"gas\"]:\n",
    "        if target not in df_train.columns:\n",
    "            continue\n",
    "\n",
    "        prophet_model = fit_prophet(df_train, target)\n",
    "        for test_file in test_files:\n",
    "            df_test = preprocess_data(pd.read_csv(os.path.join(INPUT_DIR, test_file)))\n",
    "            df_test = df_test.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "            if target not in df_test.columns:\n",
    "                continue\n",
    "            \n",
    "            # Length of test set\n",
    "            test_len = len(df_test)\n",
    "            if test_len == 0:\n",
    "                continue\n",
    "            \n",
    "            # Prophet forecasting approach:\n",
    "            #   We create a future dataframe for the test horizon\n",
    "            #   For hourly data, we do freq='h' \n",
    "            df_train_index = df_train.index\n",
    "            last_train_timestamp = df_train_index.max()\n",
    "\n",
    "            # Make a future DataFrame of length test_len hours beyond last train timestamp\n",
    "            future_dates = pd.date_range(start=last_train_timestamp, periods=test_len+1, freq='h')[1:]\n",
    "            \n",
    "            # Prophet needs a DataFrame with 'ds' column\n",
    "            future_df = pd.DataFrame({'ds': future_dates})\n",
    "            \n",
    "            forecast = prophet_model.predict(future_df)\n",
    "            y_pred = forecast['yhat'].values  # predicted values\n",
    "            # True test values\n",
    "            y_true = df_test[target].values\n",
    "            \n",
    "            rmse = evaluate_model(y_true, y_pred)\n",
    "            if rmse is not None:\n",
    "                rmse_dict[target].append(rmse)\n",
    "\n",
    "    for t in rmse_dict:\n",
    "        rmse_dict[t] = np.mean(rmse_dict[t]) if rmse_dict[t] else None\n",
    "    return rmse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Models' Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} # store models' performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating ARIMA Model...\")\n",
    "try:\n",
    "    arima_results = evaluate_arima()\n",
    "    results[\"ARIMA\"] = arima_results\n",
    "except Exception as e:\n",
    "    print(f\"ARIMA evaluation failed: {e}\")\n",
    "    results[\"ARIMA\"] = {\"elec\": None, \"gas\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Sarimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating SARIMAX Model...\")\n",
    "try:\n",
    "    sarimax_results = evaluate_sarimax()\n",
    "    results[\"SARIMAX\"] = sarimax_results\n",
    "except Exception as e:\n",
    "    print(f\"SARIMAX evaluation failed: {e}\")\n",
    "    results[\"SARIMAX\"] = {\"elec\": None, \"gas\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 LSTM (Single Household)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Single-Household LSTM...\")\n",
    "try:\n",
    "    lstm_single_results = evaluate_lstm_single()\n",
    "    results[\"LSTM Single\"] = lstm_single_results\n",
    "except Exception as e:\n",
    "    print(f\"LSTM Single evaluation failed: {e}\")\n",
    "    results[\"LSTM Single\"] = {\"elec\": None, \"gas\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Sequential LSTM (20 Households)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating LSTM Sequential Training...\")\n",
    "try:\n",
    "    lstm_seq_results = evaluate_lstm_sequential()\n",
    "    results[\"LSTM Sequential\"] = lstm_seq_results\n",
    "except Exception as e:\n",
    "    print(f\"LSTM Sequential evaluation failed: {e}\")\n",
    "    results[\"LSTM Sequential\"] = {\"elec\": None, \"gas\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5 Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Prophet Model...\")\n",
    "try:\n",
    "    prophet_results = evaluate_prophet()\n",
    "    results[\"PROPHET\"] = prophet_results\n",
    "except Exception as e:\n",
    "    print(f\"Prophet evaluation failed: {e}\")\n",
    "    results[\"PROPHET\"] = {\"elec\": None, \"gas\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparison of Models' Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nEvaluation Results (RMSE for Elec and Gas):\")\n",
    "for model_name, rmse_dict in results.items():\n",
    "    elec_rmse = rmse_dict.get(\"elec\", \"N/A\")\n",
    "    gas_rmse = rmse_dict.get(\"gas\", \"N/A\")\n",
    "    elec_rmse_str = f\"{elec_rmse:.3f}\" if elec_rmse not in [None, \"N/A\"] else \"N/A\"\n",
    "    gas_rmse_str = f\"{gas_rmse:.3f}\" if gas_rmse not in [None, \"N/A\"] else \"N/A\"\n",
    "    print(f\"{model_name} -> Elec RMSE: {elec_rmse_str} | Gas RMSE: {gas_rmse_str}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
